<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Scott Graham" />

<meta name="date" content="2017-12-06" />

<title>STAT 541 Final Project Report</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/lumen.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 54px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 59px;
  margin-top: -59px;
}

.section h2 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h3 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h4 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h5 {
  padding-top: 59px;
  margin-top: -59px;
}
.section h6 {
  padding-top: 59px;
  margin-top: -59px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">STAT 541 Final Project</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../../">Home</a>
</li>
<li>
  <a href="Report.html">Report</a>
</li>
<li>
  <a href="Presentation.html">Presentation</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">STAT 541 Final Project Report</h1>
<h4 class="author"><em>Scott Graham</em></h4>
<h4 class="date"><em>December 06, 2017</em></h4>

</div>


<p><span class="math display">\[
  \newcommand{\Prob}{\operatorname{P}}
  \newcommand{\E}{\operatorname{E}}
  \newcommand{\Var}{\operatorname{Var}}
  \newcommand{\Cov}{\operatorname{Cov}}
  \newcommand{\se}{\operatorname{se}}
  \newcommand{\re}{\operatorname{re}}
  \newcommand{\ybar}{{\overline{Y}}}
  \newcommand{\phat}{{\hat{p}}}
  \newcommand{\that}{{\hat{T}}}
  \newcommand{\med}{{\tilde{Y}}}
\]</span></p>
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<div id="data" class="section level2">
<h2>Data</h2>
<p>This data set contains the results of 217 games played in the video games NHL 12, 13, and 17, played between 2 friends, Tim and Randy (P1 and P2 respectively)<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>. They compiled a incredibly detailed gamelog for each game played, with well over 50 variables recorded, and period by period breakdowns. For those not familiar, the NHL series of video games is developed by EA Sports, and serves as the most readily available means of simulating real NHL.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> While obviously not perfect, it serves as a decent proxy for predicting the results of hockey games in general, as many of the same principles apply.</p>
<p>Since the outcome of a game is determined by the end score, any variables referencing the goals scored have been removed, as they serve little to know interest in the prediction of games. However there is one exception, and that is who scored the first goal of the game, which in itself seemed to be an interesting possible predictor. Instead more periphery statistics will be looked at, such as hits, shots, face off percentage, breakaways, penalties, and other such things. As well, a 1 game lagged result term has been included, to see if any autocorrelation exists within the results, and if it plays a significant role in the prediction process.</p>
</div>
<div id="analysis" class="section level2">
<h2>Analysis</h2>
<p>In this report, a Binomial and Log Linear LASSO model will be used in the variable selection and coefficient estimation process, in addition to the usual step wise methods. As previously mentioned, the autocorrelation of the results will also be examined to see if such an effect exists. Finally a variety of visual tools will be used in the exploratory analysis portion of the paper, in an effort to understand the relationships within the data.</p>
<p>For the LASSO model, the package <code>glmnet</code> is used, to perform an elastic-net version of the LASSO model<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>. All the data cleaning is done with the usage of a variety of packages from the “tidyverse”. This includes the packages <code>dplyr</code>, <code>tidyr</code>, <code>magrittr</code>, and <code>tibble</code>.<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> As well, the <code>flatr</code> package is used to transform contingency tables into “tidy” data frames, where each variable has its own column for each observation. <code>flatr</code> was created by myself in an effort to simplify some of the cleaning done for this course. Finally <code>ggplot2</code> is used as the primary visualization method, due to its flexibility in creating visualizations, and overall better appearance than base graphics.</p>
</div>
<div id="research-question" class="section level2">
<h2>Research Question</h2>
<p>What periphery (non-goal related) statistics can be used to predict the outcome of a hockey game?</p>
</div>
</div>
<div id="exploratory-analysis" class="section level1">
<h1>Exploratory Analysis</h1>
<div id="correlation-matrix" class="section level2">
<h2>Correlation Matrix</h2>
<p>One of the primary issues with any regression model is collinearity among the predictors. To analyse this, a correlation matrix has been generated, presented as the corresponding visualization. Any of the variables that are of numeric type are evaluated.</p>
<pre class="r"><code>hockey_data %&gt;%
  select_if(.predicate = is.numeric) %&gt;%
  select(
    -c(
      Game
      ,Version
      ,`P2 - P1 Goals`
    )
    ,-ends_with(&quot;Differential&quot;)
  ) %&gt;% 
  cor() %&gt;% 
  as.data.frame() %&gt;%  
  rownames_to_column() %&gt;% 
  as.tibble() %&gt;% 
  gather(
    key = Column
    ,value = Correlation
    ,-rowname
  ) %&gt;% 
  rename(Row = rowname) %&gt;% 
  ggplot(
    aes(
      x = Column
      ,y = Row
      ,fill = Correlation
    )
  ) +
  geom_raster() +
  scale_fill_distiller(
    type = &quot;div&quot;
    ,palette = &quot;RdBu&quot;
    ,limits = c(-1, 1)
  ) +
  theme(
    axis.text.x = element_text(angle = 90, hjust = 1)
    ,axis.title.x = element_blank()
    ,axis.title.y = element_blank()
  )</code></pre>
<p><img src="Report_files/figure-html/Cor%20Matrix-1.png" width="672" /></p>
<p>There appears to be be fairly small correlation among the different variables, with a few exceptions. Time of Attack (ToA) and Shots appear to have a somewhat significant correlation, which is expected, as the more time spent in the attacking zone, the more shots are generated. Hits and face offs appear to be correlated, which is interesting, as I can’t see a reason why this would occur.</p>
</div>
<div id="distribution-of-goals" class="section level2">
<h2>Distribution of Goals</h2>
<p>The underlying distribution of goals may also be of interest. I expect a right-skewed distribution, as most games typically have 3.2972 goals for each player. As well, the domain of the distribution is <span class="math inline">\([0, \infty)\)</span>, as negative goals can’t be awarded. The distribution of their difference is looked at as well, which should be bimodal, as no ties exist in the data set or in the modern NHL.</p>
<pre class="r"><code>hockey_data %&gt;% 
  select(
    P1
    ,P2
    ,`P2 - P1 Goals`
  ) %&gt;% 
  gather(
    key = &quot;Variable&quot;
    ,value = &quot;Goals&quot;
  ) %&gt;% 
  mutate(
    Variable = 
      factor(
        x = Variable
        ,levels = c(&quot;P1&quot;, &quot;P2&quot;, &quot;P2 - P1 Goals&quot;)
      )
  ) %&gt;% 
  ggplot(
    aes(
      x = Goals
      ,colour = Variable
    )
  ) +
  geom_density() +
  facet_wrap(
    facet = ~ Variable
    ,ncol = 1
  ) +
  scale_colour_brewer(
    type = &quot;qual&quot;
    ,name = &quot;Variable&quot;
    ,palette = &quot;Set2&quot;
  )</code></pre>
<p><img src="Report_files/figure-html/Goal%20Distribution-1.png" width="672" /></p>
<p>P2’s distribution matches what was expected, as well as the difference. P1’s on the other hand appears to be more symmetric than expected, possibly due to a decreased likelihood of him running up the score.</p>
<p>Another factor to explore is if the version of the game changes the distribution. Besides updating player rosters, various game play changes are made, which may effect each player differently.</p>
<pre class="r"><code>hockey_data %&gt;% 
  select(
    P1
    ,P2
    ,`P2 - P1 Goals`
    ,Version
  ) %&gt;% 
  gather(
    key = &quot;Variable&quot;
    ,value = &quot;Goals&quot;
    ,-Version
  ) %&gt;% 
  mutate(
    Variable = 
      factor(
        x = Variable
        ,levels = c(&quot;P1&quot;, &quot;P2&quot;, &quot;P2 - P1 Goals&quot;)
      )
  ) %&gt;% 
  ggplot(
    aes(
      x = Goals
      ,colour = Variable
    )
  ) +
  geom_density() +
  facet_grid(facet = Version ~ Variable) +
  scale_colour_brewer(
    type = &quot;qual&quot;
    ,name = &quot;Variable&quot;
    ,palette = &quot;Set2&quot;
  )</code></pre>
<p><img src="Report_files/figure-html/Goal%20Distribution%20by%20Version-1.png" width="672" /></p>
<p>It appears version does not play a significant role in determining the distribution of goals. This can be seen by reading the chart from top to bottom, and not seeing significant changes in the shape of the kernel density estimators.</p>
</div>
<div id="goals-vs.shots" class="section level2">
<h2>Goals vs. Shots</h2>
<blockquote>
<p>“You miss 100% of the shots you don’t take.” - Wayne Gretzky</p>
</blockquote>
<p>An important relationship in hockey is that of Goals and Shots. Reasonably assumed, the more shots you take, the more goals you score. The question is though, does this relationship differ for each player?</p>
<pre class="r"><code>hockey_data_GvS &lt;- 
  hockey_data %&gt;% 
  select(
    Game
    ,`P1 Goals` = P1
    ,`P2 Goals` = P2
    ,`P1 Shots`
    ,`P2 Shots`
  ) %&gt;% 
  gather(
    key = Variable
    ,value = Value
    ,-Game
  ) %&gt;% 
  mutate(
    Player = 
      if_else(
        grepl(pattern = &quot;P1 *&quot;, x = .$Variable), &quot;P1&quot;, &quot;P2&quot;
      )
    ,Statistic =
      if_else(
        grepl(pattern = &quot;* Goals&quot;, x = .$Variable), &quot;Goals&quot;, &quot;Shots&quot;
      )
  ) %&gt;% 
  select(-Variable) %&gt;% 
  spread(
    key = Statistic
    ,value = Value
  ) %&gt;% 
  mutate(Player = as.factor(Player))

hockey_data_GvS %&gt;% 
  ggplot(
    aes(
      x = Shots
      ,y = Goals
      ,colour = Player
    )
  ) +
  geom_point(
    data = 
      hockey_data_GvS %&gt;% 
      select(-Player)
    ,colour=&quot;grey92&quot;
    ,alpha = 0.75
  ) +
  geom_point() +
  geom_smooth(
    method = &quot;loess&quot;
    ,se = FALSE
  ) +
  geom_smooth(
    method = &quot;lm&quot;
    ,se = FALSE
  )  +
  facet_grid(facet = ~ Player) +
  scale_colour_brewer(
    type = &quot;qual&quot;
    ,name = &quot;Player&quot;
    ,palette = &quot;Set2&quot;
  )</code></pre>
<p><img src="Report_files/figure-html/GvS-1.png" width="672" /></p>
<p>As one can see, clearly P2 scores more goals on average (4.106 vs. 2.4885). However the slopes of the two lines appear to differ, with P1’s being higher than P2’s. This leads to two possible conclusions:</p>
<ol style="list-style-type: decimal">
<li>P1 should shoot the puck more often, as his expected goals will rise at a faster rate than P2’s</li>
<li>P2 shouldn’t worry as much about getting lots of shots, but primarily focus on getting high danger chances (chances close to the net)</li>
</ol>
</div>
<div id="autocorrelation" class="section level2">
<h2>Autocorrelation</h2>
<p>Autocorrelation measures the correlation between <span class="math inline">\(\left(X_{t-k}, X_{t}\right), t=k, k+1\dots,n\)</span>. It is primarily used in timeseries analysis to measure the tendency for a time series to be mean reverting (negative autocorrelation) or a tendency to go on long runs in one direct or the other (positive autocorrelation). This is primarily used in the analysis of stock returns, but can be applied to any time series.</p>
<p>What we wish to measure is if P1 wins the last game, is he more or less likely to win the following game? This can be accomplished both graphically, and through the use of a Durbin-Watson test<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>. More specifically, the Durbin-Watson statistic tests: <span class="math display">\[
  H_{0}: \text{Autocorrelation} = 0
\]</span> <span class="math display">\[
  H_{1}: \text{Autocorrelation} \neq 0
\]</span></p>
<pre class="r"><code>hockey_data_acf &lt;- 
  hockey_data %&gt;%
  mutate(P1_Win_Bool = if_else(`P1 Win` == &quot;Yes&quot;, 1, 0)) %&gt;% 
  select(P1_Win_Bool) %&gt;% 
  acf(plot = FALSE)

hockey_data_acf</code></pre>
<pre><code>## 
## Autocorrelations of series &#39;.&#39;, by lag
## 
##      0      1      2      3      4      5      6      7      8      9 
##  1.000  0.101  0.126 -0.004 -0.057  0.019 -0.047  0.105 -0.070 -0.034 
##     10     11     12     13     14     15     16     17     18     19 
## -0.157  0.033 -0.065 -0.015 -0.119 -0.044 -0.013 -0.086 -0.036 -0.057 
##     20     21     22     23 
##  0.050 -0.105  0.079 -0.050</code></pre>
<pre class="r"><code>data.frame(Lag = hockey_data_acf$lag, Autocorrelation = hockey_data_acf$acf) %&gt;% 
  ggplot(
    aes(
      x = Lag
      ,y = Autocorrelation
    )
  ) +
  geom_hline(
    aes(
      yintercept = 0
    )
    ,linetype = &quot;dotted&quot;
  ) +
  geom_segment(
    aes(
      xend = Lag
      ,yend = 0
    )
  ) +
  geom_smooth(
    linetype = 0
    ,method = &quot;lm&quot;
    ,formula = y ~ 1
  )</code></pre>
<p><img src="Report_files/figure-html/Autocorrelation%20Plot-1.png" width="672" /></p>
<p>As one can see, the autocorrelation for <span class="math inline">\(k=1\)</span> is not larger than the 95% confidence interval, as shown by the grey band. Since we are interested in the case for <span class="math inline">\(k=1\)</span>, we can confirm this by modeling: <span class="math display">\[
  \operatorname{Logit}\left( \text{P1 Wins} \right) =
  \alpha + \beta_{1}\text{Previous Game&#39;s Result}
\]</span></p>
<p>and looking at the Wald Statistic, and the Durbin-Watson Statistic from the <code>lmtest</code> package, using the function <code>dwtest</code>.</p>
<pre class="r"><code>hockey_acf_logit &lt;- 
  hockey_data %&gt;%
  glm(
    formula = `P1 Win` ~ `P1 Previous Game`
    ,family = binomial
    ,data = .
  )
summary(hockey_acf_logit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = `P1 Win` ~ `P1 Previous Game`, family = binomial, 
##     data = .)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.8677  -0.6905  -0.6905  -0.6905   1.7610  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)            -1.3122     0.1904  -6.891 5.55e-12 ***
## `P1 Previous Game`Yes   0.5294     0.3568   1.484    0.138    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.11  on 215  degrees of freedom
## Residual deviance: 233.98  on 214  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 237.98
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>lmtest::dwtest(hockey_acf_logit, alternative = &quot;two.sided&quot;)</code></pre>
<pre><code>## 
##  Durbin-Watson test
## 
## data:  hockey_acf_logit
## DW = 2.024, p-value = 0.8647
## alternative hypothesis: true autocorrelation is not 0</code></pre>
<p>The Wald Statistic allows us to reject the null hypothesis that the previous games result has an effect on the game at hand, and the Durbin-Watson Statistics also confirms this result, both based on the sample.</p>
</div>
</div>
<div id="modeling" class="section level1">
<h1>Modeling</h1>
<div id="contingency-table-model" class="section level2">
<h2>Contingency Table Model</h2>
<div id="contingency-table" class="section level3">
<h3>Contingency Table</h3>
<p>A very simplistic model is considered as a starting point for the modeling process, as well as a means of showing of the 3 main functions of the <code>flatr</code> package. The contingency table has 3 dimensions; Winner, Home Team, and who scored 1st.</p>
<pre class="r"><code>hockey_ct_summary &lt;- 
  hockey_data %&gt;% 
  select(
    `P1 Win`
    ,`P1 Home`
    ,`Scored 1st`
  ) %&gt;% 
  group_by_all() %&gt;% 
  count() %&gt;% 
  ungroup()

hockey_ct &lt;- 
  array(
    data =
      c(
        hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;Yes&quot;
            ,`P1 Home` == &quot;H&quot;
            ,`Scored 1st` == &quot;P1&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;Yes&quot;
            ,`P1 Home` == &quot;H&quot;
            ,`Scored 1st` == &quot;P2&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;No&quot;
            ,`P1 Home` == &quot;H&quot;
            ,`Scored 1st` == &quot;P1&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;No&quot;
            ,`P1 Home` == &quot;H&quot;
            ,`Scored 1st` == &quot;P2&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;Yes&quot;
            ,`P1 Home` == &quot;A&quot;
            ,`Scored 1st` == &quot;P1&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;Yes&quot;
            ,`P1 Home` == &quot;A&quot;
            ,`Scored 1st` == &quot;P2&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;No&quot;
            ,`P1 Home` == &quot;A&quot;
            ,`Scored 1st` == &quot;P1&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
        ,hockey_ct_summary %&gt;% 
          filter(
            `P1 Win` == &quot;No&quot;
            ,`P1 Home` == &quot;A&quot;
            ,`Scored 1st` == &quot;P2&quot;
          ) %&gt;% 
          select(n) %&gt;% 
          as.numeric()
      )
    ,dim = c(2, 2, 2)
    ,dimnames = 
      list(
        Scored_1st = c(&quot;P1&quot;, &quot;P2&quot;)
        ,Winner = c(&quot;P1&quot;, &quot;P2&quot;)
        ,Home = c(&quot;P1&quot;, &quot;P2&quot;)
      )
  )
hockey_ct</code></pre>
<pre><code>## , , Home = P1
## 
##           Winner
## Scored_1st P1 P2
##         P1 12 12
##         P2  4 23
## 
## , , Home = P2
## 
##           Winner
## Scored_1st P1 P2
##         P1 25 35
##         P2 10 96</code></pre>
</div>
<div id="flatten_ct" class="section level3">
<h3>flatten_ct</h3>
<p><code>flatten_ct</code> takes a <span class="math inline">\(i \times j \times k\)</span> contingency table, and turns it into a data frame. The 3 columns of the data frame are populated by the names of the levels of the table, and are repeated the appropriate number of times. It also plays well with <code>magrittr</code>’s <code>%&gt;%</code>, or pipe. It is equivalent to writing nested functions. For example <code>10 %&gt;% rnorm() %&gt;% mean()</code> is equivalent to <code>mean(rnorm(10))</code>.</p>
<p>Below is a sample of what the output of the function, and proof it matches the contingency table when summarized.</p>
<pre class="r"><code>hockey_ct %&gt;% 
  flatten_ct() %&gt;% 
  head() %&gt;% 
  kable(digits = 4, format = &quot;latex&quot;, booktabs = T) %&gt;% 
  kable_styling()</code></pre>

<pre class="r"><code>hockey_ct %&gt;% 
  flatten_ct() %&gt;% 
  group_by_all() %&gt;% 
  count() %&gt;% 
  kable(digits = 4, format = &quot;latex&quot;, booktabs = T) %&gt;% 
  kable_styling()</code></pre>

</div>
<div id="regression-models-and-goodness-of-fit-tests" class="section level3">
<h3>Regression Models and Goodness of Fit Tests</h3>
<p>A Logistic regression model has been fitted, where: <span class="math display">\[
  \operatorname{Logit}\left( \text{P1 Wins} \right) =
  \alpha + \beta_{1}\text{Scored First} + \beta_{2}\text{Home Team}
\]</span></p>
<p>As well a log linear function is fitted, where: <span class="math display">\[
  \ln(\mu) =
  \alpha + \beta_{1}\text{Scored First} + \beta_{2}\text{P1 Wins} + \beta_{3}\text{Home Team}
\]</span></p>
<pre class="r"><code>hockey_ct_logit &lt;- 
  hockey_ct %&gt;% 
  flatten_ct() %&gt;% 
  glm(
    Winner ~ Scored_1st + Home
    ,family = binomial
    ,data = .
  )
summary(hockey_ct_logit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Winner ~ Scored_1st + Home, family = binomial, 
##     data = .)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1606   0.4515   0.4515   0.5451   1.1967  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)   -0.0454     0.3525  -0.129    0.898    
## Scored_1stP2   1.8769     0.3591   5.227 1.72e-07 ***
## HomeP2         0.4006     0.3883   1.032    0.302    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.65  on 216  degrees of freedom
## Residual deviance: 203.72  on 214  degrees of freedom
## AIC: 209.72
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>goodness_of_fit(model = hockey_ct_logit, type = &quot;Chisq&quot;)</code></pre>
<pre><code>## 
## Chi-squared Goodness of Fit Test 
## 
## model: hockey_ct_logit 
## Chi-squared = 0.04855, df = 1, p-value = 0.82561</code></pre>
<pre class="r"><code>goodness_of_fit(model = hockey_ct_logit, type = &quot;Gsq&quot;)</code></pre>
<pre><code>## 
## G-squared Goodness of Fit Test 
## 
## model: hockey_ct_logit 
## G-squared = 0.04815, df = 1, p-value = 0.82632</code></pre>
<pre class="r"><code>hockey_ct_loglin &lt;- 
  hockey_ct %&gt;% 
  flatten_ct() %&gt;% 
  group_by_all() %&gt;% 
  count() %&gt;% 
  ungroup() %&gt;% 
  glm(
    n ~ (Scored_1st + Winner + Home)^2
    ,family = poisson
    ,data = .
  )
summary(hockey_ct_loglin)</code></pre>
<pre><code>## 
## Call:
## glm(formula = n ~ (Scored_1st + Winner + Home)^2, family = poisson, 
##     data = .)
## 
## Deviance Residuals: 
##        1         2         3         4         5         6         7  
## -0.07803   0.05466   0.07922  -0.04591   0.13938  -0.08535  -0.05656  
##        8  
##  0.02782  
## 
## Coefficients:
##                       Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)             2.5074     0.2671   9.387  &lt; 2e-16 ***
## Scored_1stP2           -1.1916     0.4036  -2.953  0.00315 ** 
## WinnerP2               -0.0454     0.3525  -0.129  0.89754    
## HomeP2                  0.7006     0.3150   2.224  0.02614 *  
## Scored_1stP2:WinnerP2   1.8769     0.3591   5.227 1.73e-07 ***
## Scored_1stP2:HomeP2     0.3131     0.3527   0.888  0.37472    
## WinnerP2:HomeP2         0.4006     0.3883   1.032  0.30227    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 174.425496  on 7  degrees of freedom
## Residual deviance:   0.048146  on 1  degrees of freedom
## AIC: 51.99
## 
## Number of Fisher Scoring iterations: 3</code></pre>
<pre class="r"><code>goodness_of_fit_loglin(model = hockey_ct_loglin, type = &quot;Chisq&quot;)</code></pre>
<pre><code>## 
## Chi-squared Goodness of Fit Test 
## 
## model: hockey_ct_loglin 
## Chi-squared = 0.04855, df = 1, p-value = 0.82561</code></pre>
<pre class="r"><code>goodness_of_fit_loglin(model = hockey_ct_loglin, type = &quot;Gsq&quot;)</code></pre>
<pre><code>## 
## G-squared Goodness of Fit Test 
## 
## model: hockey_ct_loglin 
## G-squared = 0.04815, df = 1, p-value = 0.82632</code></pre>
<p><code>goodness_of_fit</code> and <code>goodness_of_fit_loglin</code> perform the goodness of fit tests for binomial models and log linear models respectively. Eventually I plan on putting them both in the same function call. As well, they have a nice looking printout, detailing the model, test statistic, degrees of freedom and the p-value.</p>
<p>The models themselves aren’t terribly interesting, as there aren’t many statistically significant coefficients. However both of them fail to reject the null hypothesis of the model fits the data in the goodness of fit tests, so at least that’s a start.</p>
</div>
</div>
<div id="full-model" class="section level2">
<h2>Full Model</h2>
<p>The full model used is the model that contains the majority of the predictors.</p>
<pre class="r"><code>hockey_full_logit_01&lt;- 
  hockey_data %&gt;% 
  glm(
    formula = 
      `P1 Win` ~
      `P1 Previous Game` + `P1 Home` + `P1 Team` + `Scored 1st` + 
      `P1 Shots` + `P2 Shots` + `P1 Hits` + `P2 Hits` + `P1 ToA` + `P2 ToA` + 
      `P1 Passing` + `P2 Passing` + `P1 Faceoffs` + `P2 Faceoffs` + 
      `P1 Offensive Faceoffs` + `P2 Offensive Faceoffs` + `P1 PM` + `P2 PM` + 
      `P1 Powerplays` + `P2 Powerplays` + `P1 Penalty Shots` + `P2 Penalty Shots` + 
      `P1 Breakaways` + `P2 Breakaways`
    ,family = binomial
    ,data = .
  )
summary(hockey_full_logit_01)</code></pre>
<pre><code>## 
## Call:
## glm(formula = `P1 Win` ~ `P1 Previous Game` + `P1 Home` + `P1 Team` + 
##     `Scored 1st` + `P1 Shots` + `P2 Shots` + `P1 Hits` + `P2 Hits` + 
##     `P1 ToA` + `P2 ToA` + `P1 Passing` + `P2 Passing` + `P1 Faceoffs` + 
##     `P2 Faceoffs` + `P1 Offensive Faceoffs` + `P2 Offensive Faceoffs` + 
##     `P1 PM` + `P2 PM` + `P1 Powerplays` + `P2 Powerplays` + `P1 Penalty Shots` + 
##     `P2 Penalty Shots` + `P1 Breakaways` + `P2 Breakaways`, family = binomial, 
##     data = .)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9727  -0.6721  -0.3249  -0.0002   2.5448  
## 
## Coefficients: (1 not defined because of singularities)
##                           Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)             -1.096e+01  4.927e+00  -2.224   0.0261 *  
## `P1 Previous Game`Yes    2.597e-01  4.674e-01   0.556   0.5785    
## `P1 Home`H                      NA         NA      NA       NA    
## `P1 Team`Detroit        -1.334e+01  1.797e+03  -0.007   0.9941    
## `P1 Team`Montreal        6.947e-01  1.221e+00   0.569   0.5694    
## `P1 Team`Nashville      -1.205e+01  3.956e+03  -0.003   0.9976    
## `P1 Team`Philadelphia   -1.382e+01  2.687e+03  -0.005   0.9959    
## `P1 Team`Philly         -6.436e-01  2.187e+00  -0.294   0.7686    
## `P1 Team`Pittsburgh     -9.713e-01  1.681e+00  -0.578   0.5635    
## `P1 Team`San Jose        1.285e+00  1.939e+00   0.663   0.5075    
## `P1 Team`Tampa Bay      -1.712e+01  3.956e+03  -0.004   0.9965    
## `P1 Team`Washington     -1.386e+01  3.956e+03  -0.004   0.9972    
## `P1 Team`Winnipeg       -1.269e+01  3.956e+03  -0.003   0.9974    
## `Scored 1st`P2          -1.987e+00  4.679e-01  -4.247 2.17e-05 ***
## `P1 Shots`               6.916e-03  2.556e-02   0.271   0.7868    
## `P2 Shots`              -1.239e-02  3.154e-02  -0.393   0.6945    
## `P1 Hits`                8.494e-03  4.551e-02   0.187   0.8519    
## `P2 Hits`                4.119e-02  3.600e-02   1.144   0.2525    
## `P1 ToA`                -1.230e-01  3.242e-01  -0.379   0.7044    
## `P2 ToA`                -7.954e-03  3.182e-01  -0.025   0.9801    
## `P1 Passing`             9.043e+00  4.600e+00   1.966   0.0493 *  
## `P2 Passing`             7.841e+00  4.963e+00   1.580   0.1141    
## `P1 Faceoffs`           -7.983e-02  5.477e-02  -1.458   0.1449    
## `P2 Faceoffs`           -5.485e-02  6.520e-02  -0.841   0.4002    
## `P1 Offensive Faceoffs`  9.389e-02  1.091e-01   0.860   0.3897    
## `P2 Offensive Faceoffs`  4.889e-02  1.055e-01   0.463   0.6432    
## `P1 PM`                  1.089e-01  1.089e-01   1.000   0.3174    
## `P2 PM`                 -3.442e-02  1.036e-01  -0.332   0.7398    
## `P1 Powerplays`          3.582e-01  2.443e-01   1.466   0.1426    
## `P2 Powerplays`         -2.163e-01  2.670e-01  -0.810   0.4178    
## `P1 Penalty Shots`       2.718e-01  4.455e-01   0.610   0.5417    
## `P2 Penalty Shots`      -1.132e+00  8.147e-01  -1.389   0.1648    
## `P1 Breakaways`          2.593e-01  1.378e-01   1.881   0.0600 .  
## `P2 Breakaways`         -5.875e-02  1.527e-01  -0.385   0.7004    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.11  on 215  degrees of freedom
## Residual deviance: 169.41  on 183  degrees of freedom
##   (1 observation deleted due to missingness)
## AIC: 235.41
## 
## Number of Fisher Scoring iterations: 16</code></pre>
<p>First thing to notice is that R doesn’t like the P1 Home variable, and defines it as a singularity, so it should be removed. As well, P1 Team is taken out, as it makes the model much more complicated, while adding no statistically significant benefit. As well, thanks to the analysis on the autocorrelation term, we can remove that as well. A model with interaction terms would be useful to look at, however due to the number of parameter, this is not feasible as <span class="math inline">\(p&gt;n\)</span>.</p>
<pre class="r"><code>hockey_full_logit_02&lt;- 
  glm(
    formula = 
      `P1 Win` ~
      `Scored 1st` + 
      `P1 Shots` + `P2 Shots` +
      `P1 Hits` + `P2 Hits` +
      `P1 ToA` + `P2 ToA` +
      `P1 Passing` + `P2 Passing` + 
      `P1 Faceoffs` + `P2 Faceoffs` +
      `P1 Offensive Faceoffs` + `P2 Offensive Faceoffs` +
      `P1 PM` + `P2 PM` +
      `P1 Powerplays` + `P2 Powerplays` +
      `P1 Penalty Shots` + `P2 Penalty Shots` +
      `P1 Breakaways` + `P2 Breakaways`
    ,family = binomial
    ,data =   hockey_data
  )
summary(hockey_full_logit_02)</code></pre>
<pre><code>## 
## Call:
## glm(formula = `P1 Win` ~ `Scored 1st` + `P1 Shots` + `P2 Shots` + 
##     `P1 Hits` + `P2 Hits` + `P1 ToA` + `P2 ToA` + `P1 Passing` + 
##     `P2 Passing` + `P1 Faceoffs` + `P2 Faceoffs` + `P1 Offensive Faceoffs` + 
##     `P2 Offensive Faceoffs` + `P1 PM` + `P2 PM` + `P1 Powerplays` + 
##     `P2 Powerplays` + `P1 Penalty Shots` + `P2 Penalty Shots` + 
##     `P1 Breakaways` + `P2 Breakaways`, family = binomial, data = hockey_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9680  -0.6815  -0.3479  -0.1289   2.5973  
## 
## Coefficients:
##                          Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)             -9.882440   4.551421  -2.171   0.0299 *  
## `Scored 1st`P2          -1.964100   0.445035  -4.413 1.02e-05 ***
## `P1 Shots`               0.013922   0.024908   0.559   0.5762    
## `P2 Shots`              -0.016038   0.029605  -0.542   0.5880    
## `P1 Hits`               -0.007772   0.041855  -0.186   0.8527    
## `P2 Hits`                0.043846   0.032562   1.347   0.1781    
## `P1 ToA`                -0.209459   0.313304  -0.669   0.5038    
## `P2 ToA`                -0.052093   0.306604  -0.170   0.8651    
## `P1 Passing`             8.197580   4.273661   1.918   0.0551 .  
## `P2 Passing`             9.170535   4.748918   1.931   0.0535 .  
## `P1 Faceoffs`           -0.061720   0.048063  -1.284   0.1991    
## `P2 Faceoffs`           -0.066995   0.057201  -1.171   0.2415    
## `P1 Offensive Faceoffs`  0.072268   0.100372   0.720   0.4715    
## `P2 Offensive Faceoffs`  0.071543   0.101292   0.706   0.4800    
## `P1 PM`                  0.050668   0.087914   0.576   0.5644    
## `P2 PM`                  0.005239   0.086114   0.061   0.9515    
## `P1 Powerplays`          0.249659   0.211105   1.183   0.2370    
## `P2 Powerplays`         -0.066562   0.217815  -0.306   0.7599    
## `P1 Penalty Shots`       0.226442   0.418251   0.541   0.5882    
## `P2 Penalty Shots`      -1.324944   0.805789  -1.644   0.1001    
## `P1 Breakaways`          0.227003   0.129030   1.759   0.0785 .  
## `P2 Breakaways`         -0.041275   0.147986  -0.279   0.7803    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.65  on 216  degrees of freedom
## Residual deviance: 175.63  on 195  degrees of freedom
## AIC: 219.63
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<pre class="r"><code>goodness_of_fit(hockey_full_logit_02, type = &quot;Chisq&quot;)</code></pre>
<pre><code>## 
## Chi-squared Goodness of Fit Test 
## 
## model: hockey_full_logit_02 
## Chi-squared = 197.2851, df = 195, p-value = 0.44078</code></pre>
<p>From this we can see that we have two significant parameters at <span class="math inline">\(\alpha=0.05\)</span>, and a couple that are close. As well, we can examine the model that only consists of the summed version of the predictors:</p>
<pre class="r"><code>hockey_full_logit_03&lt;- 
  glm(
    formula = 
      `P1 Win` ~
      Shots + Hits + ToA +
      Passing + Faceoffs + `Offensive Faceoffs` +
      PM + Powerplays + `Penalty Shots` + Breakaways
    ,family = binomial
    ,data =   hockey_data
  )
summary(hockey_full_logit_03)</code></pre>
<pre><code>## 
## Call:
## glm(formula = `P1 Win` ~ Shots + Hits + ToA + Passing + Faceoffs + 
##     `Offensive Faceoffs` + PM + Powerplays + `Penalty Shots` + 
##     Breakaways, family = binomial, data = hockey_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.2274  -0.7810  -0.6014  -0.3266   2.0747  
## 
## Coefficients:
##                        Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)          -7.987e+00  3.537e+00  -2.258   0.0239 *
## Shots                 5.606e-05  1.720e-02   0.003   0.9974  
## Hits                  3.196e-02  2.088e-02   1.531   0.1259  
## ToA                  -1.102e-01  2.074e-01  -0.532   0.5950  
## Passing               6.133e+00  2.515e+00   2.439   0.0147 *
## Faceoffs             -5.143e-02  2.142e-02  -2.401   0.0163 *
## `Offensive Faceoffs`  7.981e-02  6.330e-02   1.261   0.2074  
## PM                   -1.882e-03  1.705e-02  -0.110   0.9121  
## Powerplays            8.546e-02  7.343e-02   1.164   0.2445  
## `Penalty Shots`      -3.159e-01  2.807e-01  -1.125   0.2605  
## Breakaways            6.545e-02  7.864e-02   0.832   0.4052  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.65  on 216  degrees of freedom
## Residual deviance: 221.64  on 206  degrees of freedom
## AIC: 243.64
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<pre class="r"><code>goodness_of_fit(hockey_full_logit_03, type = &quot;Chisq&quot;)</code></pre>
<pre><code>## 
## Chi-squared Goodness of Fit Test 
## 
## model: hockey_full_logit_03 
## Chi-squared = 209.3532, df = 206, p-value = 0.42187</code></pre>
<p>We still only have 3 statistically significant parameters, so our model still could use some improvements. What this means we need to do some variable selection. Two methods will be looked at; Stepwise Selection, and LASSO.</p>
</div>
<div id="stepwise-selection" class="section level2">
<h2>Stepwise Selection</h2>
<p>For both directions of the stepwise selection process, BIC with <span class="math inline">\(k=\ln(217)\)</span> is used.</p>
<div id="backward-bic" class="section level3">
<h3>Backward BIC</h3>
<pre class="r"><code>hockey_full_logit_02 %&gt;% 
  step(
    direction = &quot;backward&quot;
    ,k = 
      hockey_data %&gt;% 
      count() %&gt;% 
      as.numeric() %&gt;% 
      log()
    ,trace = 0
  ) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## glm(formula = `P1 Win` ~ `Scored 1st` + `P1 Powerplays` + `P2 Penalty Shots`, 
##     family = binomial, data = hockey_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.3816  -0.6402  -0.4301  -0.2053   2.7064  
## 
## Coefficients:
##                    Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)         -0.8114     0.3667  -2.213   0.0269 *  
## `Scored 1st`P2      -1.7359     0.3675  -4.723 2.32e-06 ***
## `P1 Powerplays`      0.2133     0.0900   2.370   0.0178 *  
## `P2 Penalty Shots`  -1.5155     0.7730  -1.961   0.0499 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.65  on 216  degrees of freedom
## Residual deviance: 193.78  on 213  degrees of freedom
## AIC: 201.78
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<pre class="r"><code>hockey_step_logit_01 &lt;- 
  hockey_data %&gt;% 
  glm(
    `P1 Win` ~ `Scored 1st` + `P1 Powerplays` + `P2 Penalty Shots`
    ,family = binomial
    ,data = .
  )
goodness_of_fit(hockey_step_logit_01, type = &quot;Chisq&quot;)</code></pre>
<pre><code>## 
## Chi-squared Goodness of Fit Test 
## 
## model: hockey_step_logit_01 
## Chi-squared = 29.64574, df = 213, p-value = 1</code></pre>
<pre class="r"><code>goodness_of_fit(hockey_step_logit_01, type = &quot;Gsq&quot;)</code></pre>
<pre><code>## 
## G-squared Goodness of Fit Test 
## 
## model: hockey_step_logit_01 
## G-squared = NA, df = 213, p-value = NA</code></pre>
<p>This drastically reduces the number of parameters to 4, giving: <span class="math display">\[
  \operatorname{Logit}\left( \text{P1 Wins} \right) =
  \alpha + \beta_{1}\text{Scored 1st} + \beta_{2}\text{P1 Powerplays} + \beta_{3}\text{P2 Penalty Shots}
\]</span> <span class="math display">\[
  =
  -0.8114 - 1.7359\text{Scored 1st} + 0.2133\text{P1 Powerplays} - 1.5155\text{P2 Penalty Shots}
\]</span> Where: <span class="math display">\[
  \text{Scored 1st} =
  \begin{cases}
    0,\text{ if P1} \\
    1,\text{ if P2}
  \end{cases}
\]</span></p>
<p>And the 95% Confidence intervals for <span class="math inline">\(e^{\beta_{i}}\)</span> are:</p>
<pre class="r"><code>hockey_step_logit_01 %&gt;% 
  confint() %&gt;% 
  exp()</code></pre>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                         2.5 %    97.5 %
## (Intercept)        0.21242841 0.9011030
## `Scored 1st`P2     0.08355758 0.3556937
## `P1 Powerplays`    1.04004592 1.4827889
## `P2 Penalty Shots` 0.03360702 0.8073160</code></pre>
</div>
<div id="forward-bic" class="section level3">
<h3>Forward BIC</h3>
<pre class="r"><code>hockey_full_logit_02 %&gt;% 
  step(
    direction = &quot;forward&quot;
    ,k = 
      hockey_data %&gt;% 
      count() %&gt;% 
      as.numeric() %&gt;% 
      log()
    ,trace = 0
  ) %&gt;% 
  summary()</code></pre>
<pre><code>## 
## Call:
## glm(formula = `P1 Win` ~ `Scored 1st` + `P1 Shots` + `P2 Shots` + 
##     `P1 Hits` + `P2 Hits` + `P1 ToA` + `P2 ToA` + `P1 Passing` + 
##     `P2 Passing` + `P1 Faceoffs` + `P2 Faceoffs` + `P1 Offensive Faceoffs` + 
##     `P2 Offensive Faceoffs` + `P1 PM` + `P2 PM` + `P1 Powerplays` + 
##     `P2 Powerplays` + `P1 Penalty Shots` + `P2 Penalty Shots` + 
##     `P1 Breakaways` + `P2 Breakaways`, family = binomial, data = hockey_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9680  -0.6815  -0.3479  -0.1289   2.5973  
## 
## Coefficients:
##                          Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)             -9.882440   4.551421  -2.171   0.0299 *  
## `Scored 1st`P2          -1.964100   0.445035  -4.413 1.02e-05 ***
## `P1 Shots`               0.013922   0.024908   0.559   0.5762    
## `P2 Shots`              -0.016038   0.029605  -0.542   0.5880    
## `P1 Hits`               -0.007772   0.041855  -0.186   0.8527    
## `P2 Hits`                0.043846   0.032562   1.347   0.1781    
## `P1 ToA`                -0.209459   0.313304  -0.669   0.5038    
## `P2 ToA`                -0.052093   0.306604  -0.170   0.8651    
## `P1 Passing`             8.197580   4.273661   1.918   0.0551 .  
## `P2 Passing`             9.170535   4.748918   1.931   0.0535 .  
## `P1 Faceoffs`           -0.061720   0.048063  -1.284   0.1991    
## `P2 Faceoffs`           -0.066995   0.057201  -1.171   0.2415    
## `P1 Offensive Faceoffs`  0.072268   0.100372   0.720   0.4715    
## `P2 Offensive Faceoffs`  0.071543   0.101292   0.706   0.4800    
## `P1 PM`                  0.050668   0.087914   0.576   0.5644    
## `P2 PM`                  0.005239   0.086114   0.061   0.9515    
## `P1 Powerplays`          0.249659   0.211105   1.183   0.2370    
## `P2 Powerplays`         -0.066562   0.217815  -0.306   0.7599    
## `P1 Penalty Shots`       0.226442   0.418251   0.541   0.5882    
## `P2 Penalty Shots`      -1.324944   0.805789  -1.644   0.1001    
## `P1 Breakaways`          0.227003   0.129030   1.759   0.0785 .  
## `P2 Breakaways`         -0.041275   0.147986  -0.279   0.7803    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 236.65  on 216  degrees of freedom
## Residual deviance: 175.63  on 195  degrees of freedom
## AIC: 219.63
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Note this is the second full model discussed earlier. Since stepwise selection is almost never used in favor of other methods, and the fact that it gave us some not so useful results, LASSO will be looked at as well in hopes of a better model<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>.</p>
</div>
</div>
<div id="lasso" class="section level2">
<h2>LASSO</h2>
<p>How LASSO regression works is it estimates the value of all the parameters, and starts shrinking them to 0. In this way, LASSO has the advantage of stepwise regression in that it performs both variable selection and coefficient estimation. This is done by manipulating the bias variance trade off. By introducing a bit of bias into the coefficients, the variance of the coefficients can be reduced.</p>
<p>In LASSO regression instead of minimizing <span class="math inline">\(\left(y_{i} - \hat{y}_{i}\right)^{2}\)</span>, you minimize <span class="math inline">\(\left|y_{i} - \hat{y}_{i}\right|\)</span>. Doing so allows for coefficients to be set to 0, instead of merely near it. The question then becomes, how many coefficients are set to 0? This is accomplished through the use of a tuning parameter <span class="math inline">\(\lambda\)</span>, which is in turn determined via the usage of cross-validation. The general rule of thumb for <span class="math inline">\(\lambda\)</span> is to choose it such that the resulting model has 1 standard error than the minimum mean squared error model, in an effort to prevent overfitting.</p>
<p>To do so, the package <code>glment</code> is used, specifically the function <code>cv.glmnet</code>. Unlike base R, <code>glmnet</code> only takes matrices as inputs, so all data frames must be converted to matrices to be used with the model; <span class="math inline">\(\vec{X}\)</span> and <span class="math inline">\(\vec{Y}\)</span>.<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></p>
<pre class="r"><code>ind_var_01 &lt;- 
  c(
    &quot;Scored 1st Boolean (P2)&quot;
    ,&quot;P1 Shots&quot;
    ,&quot;P2 Shots&quot;
    ,&quot;P1 Hits&quot;
    ,&quot;P2 Hits&quot;
    ,&quot;P1 ToA&quot;
    ,&quot;P2 ToA&quot;
    ,&quot;P1 Passing&quot;
    ,&quot;P2 Passing&quot;
    ,&quot;P1 PM&quot;
    ,&quot;P2 PM&quot;
    ,&quot;P1 Powerplays&quot;
    ,&quot;P2 Powerplays&quot;
    ,&quot;P1 Faceoffs&quot;
    ,&quot;P2 Faceoffs&quot;
    ,&quot;P1 Breakaways&quot;
    ,&quot;P2 Breakaways&quot;
    ,&quot;P1 Penalty Shots&quot;
    ,&quot;P2 Penalty Shots&quot;
    ,&quot;P1 Offensive Faceoffs&quot;
    ,&quot;P2 Offensive Faceoffs&quot;
  )
ind_var_01</code></pre>
<pre><code>##  [1] &quot;Scored 1st Boolean (P2)&quot; &quot;P1 Shots&quot;               
##  [3] &quot;P2 Shots&quot;                &quot;P1 Hits&quot;                
##  [5] &quot;P2 Hits&quot;                 &quot;P1 ToA&quot;                 
##  [7] &quot;P2 ToA&quot;                  &quot;P1 Passing&quot;             
##  [9] &quot;P2 Passing&quot;              &quot;P1 PM&quot;                  
## [11] &quot;P2 PM&quot;                   &quot;P1 Powerplays&quot;          
## [13] &quot;P2 Powerplays&quot;           &quot;P1 Faceoffs&quot;            
## [15] &quot;P2 Faceoffs&quot;             &quot;P1 Breakaways&quot;          
## [17] &quot;P2 Breakaways&quot;           &quot;P1 Penalty Shots&quot;       
## [19] &quot;P2 Penalty Shots&quot;        &quot;P1 Offensive Faceoffs&quot;  
## [21] &quot;P2 Offensive Faceoffs&quot;</code></pre>
<pre class="r"><code>dep_var &lt;- &quot;P1 Win&quot;

hockey_lasso_ind_01 &lt;- 
  hockey_data %&gt;% 
  select(ind_var_01) %&gt;% 
  as.matrix()

hockey_lasso_dep &lt;-
  hockey_data %&gt;% 
  select(dep_var) %&gt;% 
  as.matrix()</code></pre>
<p>This is the list of predictor variable that is being used in the first LASSO model:</p>
<pre class="r"><code>hockey_lasso_cvfit_01 &lt;-
  cv.glmnet(
    x = hockey_lasso_ind_01
    ,y = hockey_lasso_dep
    ,family = &quot;binomial&quot;
    ,alpha = 1
  )

autoplot(hockey_lasso_cvfit_01)</code></pre>
<p><img src="Report_files/figure-html/LASSO%2001%20Plot-1.png" width="672" /></p>
<p>The above chart plots <span class="math inline">\(\ln(\lambda)\)</span> on the x-axis, the <span class="math inline">\(MSE\)</span> on the y-axis, and on the top displays the number of coefficients estimated for a give <span class="math inline">\(\lambda\)</span>. The dotted line going through the lower point is our value for <span class="math inline">\(\lambda_{min}=\)</span> 0.0306, and the dotted line going through the higher point is <span class="math inline">\(\lambda_{1se}=\)</span> 0.1125.</p>
<pre class="r"><code>hockey_lasso_coef_01_1se &lt;- coef(hockey_lasso_cvfit_01, s = &quot;lambda.1se&quot;, exact = TRUE)
hockey_lasso_coef_01_1se</code></pre>
<pre><code>## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                  1
## (Intercept)             -0.8527071
## Scored 1st Boolean (P2) -0.5670376
## P1 Shots                 .        
## P2 Shots                 .        
## P1 Hits                  .        
## P2 Hits                  .        
## P1 ToA                   .        
## P2 ToA                   .        
## P1 Passing               .        
## P2 Passing               .        
## P1 PM                    .        
## P2 PM                    .        
## P1 Powerplays            .        
## P2 Powerplays            .        
## P1 Faceoffs              .        
## P2 Faceoffs              .        
## P1 Breakaways            .        
## P2 Breakaways            .        
## P1 Penalty Shots         .        
## P2 Penalty Shots         .        
## P1 Offensive Faceoffs    .        
## P2 Offensive Faceoffs    .</code></pre>
<pre class="r"><code>hockey_lasso_coef_01_min &lt;- coef(hockey_lasso_cvfit_01, s = &quot;lambda.min&quot;, exact = TRUE)
hockey_lasso_coef_01_min</code></pre>
<pre><code>## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                    1
## (Intercept)             -2.103339281
## Scored 1st Boolean (P2) -1.398137226
## P1 Shots                 0.004405199
## P2 Shots                 .          
## P1 Hits                  .          
## P2 Hits                  .          
## P1 ToA                   .          
## P2 ToA                   .          
## P1 Passing               0.903758203
## P2 Passing               0.990987539
## P1 PM                    .          
## P2 PM                    .          
## P1 Powerplays            0.116633884
## P2 Powerplays            .          
## P1 Faceoffs              .          
## P2 Faceoffs              .          
## P1 Breakaways            0.056989459
## P2 Breakaways            .          
## P1 Penalty Shots         .          
## P2 Penalty Shots        -0.497227038
## P1 Offensive Faceoffs    .          
## P2 Offensive Faceoffs    .</code></pre>
<p>The first output is estimating coefficients using <span class="math inline">\(\lambda_{1se}\)</span>, and the latter output is with <span class="math inline">\(\lambda_{min}\)</span>. However, because cross-validation is being used in order to estimate <span class="math inline">\(\lambda\)</span>, each time the regression is run, different coefficients will arise. As a result of this, it is valuable to do this multiple times and average the results. This can be done by either averaging the coefficients each time the regression is run, or the lambdas themselves.</p>
<pre class="r"><code>reps &lt;- 100

hockey_lasso_res_01 &lt;- matrix(nrow = length(ind_var_01) + 1, ncol = reps)
rownames(hockey_lasso_res_01) &lt;- c(&quot;(Intercept)&quot;, ind_var_01)

hockey_lasso_lambda_01 &lt;- tibble(Min = as.numeric(NA), SE = as.numeric(NA))

for(i in 1:reps){
  hockey_lasso_cvfit_rep &lt;- 
    cv.glmnet(
      x = hockey_lasso_ind_01
      ,y = hockey_lasso_dep
      ,family = &quot;binomial&quot;
      ,alpha = 1
    )
  
  hockey_lasso_coef_rep_min &lt;- 
    hockey_lasso_cvfit_rep %&gt;% 
    coef(s = &quot;lambda.min&quot;, exact = TRUE) %&gt;% 
    as.matrix()
  hockey_lasso_res_01[, i] &lt;- hockey_lasso_coef_rep_min
  hockey_lasso_lambda_01[i, &quot;Min&quot;] &lt;- hockey_lasso_cvfit_rep$lambda.min
  hockey_lasso_lambda_01[i, &quot;SE&quot;] &lt;- hockey_lasso_cvfit_rep$lambda.1se
}

apply(X = hockey_lasso_res_01, MARGIN = 1, FUN = mean)</code></pre>
<pre><code>##             (Intercept) Scored 1st Boolean (P2)                P1 Shots 
##           -2.343864e+00           -1.413696e+00            4.576515e-03 
##                P2 Shots                 P1 Hits                 P2 Hits 
##           -2.228331e-05            0.000000e+00            0.000000e+00 
##                  P1 ToA                  P2 ToA              P1 Passing 
##            0.000000e+00            0.000000e+00            1.073199e+00 
##              P2 Passing                   P1 PM                   P2 PM 
##            1.215923e+00            0.000000e+00            5.687496e-05 
##           P1 Powerplays           P2 Powerplays             P1 Faceoffs 
##            1.217150e-01            0.000000e+00           -1.352674e-04 
##             P2 Faceoffs           P1 Breakaways           P2 Breakaways 
##            0.000000e+00            6.276038e-02            0.000000e+00 
##        P1 Penalty Shots        P2 Penalty Shots   P1 Offensive Faceoffs 
##            0.000000e+00           -5.293945e-01            2.589165e-05 
##   P2 Offensive Faceoffs 
##            0.000000e+00</code></pre>
<pre class="r"><code>cv.glmnet(
  x = hockey_lasso_ind_01
  ,y = hockey_lasso_dep
  ,family = &quot;binomial&quot;
  ,alpha = 1
) %&gt;% 
  coef(s = mean(hockey_lasso_lambda_01$Min))</code></pre>
<pre><code>## 22 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                                    1
## (Intercept)             -2.291872149
## Scored 1st Boolean (P2) -1.411379910
## P1 Shots                 0.004591247
## P2 Shots                 .          
## P1 Hits                  .          
## P2 Hits                  .          
## P1 ToA                   .          
## P2 ToA                   .          
## P1 Passing               1.033854992
## P2 Passing               1.158818635
## P1 PM                    .          
## P2 PM                    .          
## P1 Powerplays            0.121235931
## P2 Powerplays            .          
## P1 Faceoffs              .          
## P2 Faceoffs              .          
## P1 Breakaways            0.062261956
## P2 Breakaways            .          
## P1 Penalty Shots         .          
## P2 Penalty Shots        -0.525402691
## P1 Offensive Faceoffs    .          
## P2 Offensive Faceoffs    .</code></pre>
<p>The advantage to averaging the <span class="math inline">\(\lambda\)</span>s is that when you average the coefficients, if even just 1 time the regression finds an effect, a non-zero effect will appear for that coefficient, which could be unwanted.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><a href="#fnref1">↩</a></p></li>
<li id="fn2"><p><a href="#fnref2">↩</a></p></li>
<li id="fn3"><p><a href="#fnref3">↩</a></p></li>
<li id="fn4"><p><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p><a href="http://www.okstate.edu/sas/v8/sashtml/ets/chap8/sect5.htm" class="uri">http://www.okstate.edu/sas/v8/sashtml/ets/chap8/sect5.htm</a><a href="#fnref5">↩</a></p></li>
<li id="fn6"><p><a href="http://andrewgelman.com/2014/06/02/hate-stepwise-regression/" class="uri">http://andrewgelman.com/2014/06/02/hate-stepwise-regression/</a><a href="#fnref6">↩</a></p></li>
<li id="fn7"><p><a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#log" class="uri">https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html#log</a><a href="#fnref7">↩</a></p></li>
</ol>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
