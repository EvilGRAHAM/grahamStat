---
title: "Week 07 Questions"
author: "Scott Graham"
date: "October 27, 2017"
output:
  html_document:
    theme: lumen
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse, warn.conflicts = FALSE, quietly = TRUE)
library(magrittr, warn.conflicts = FALSE, quietly = TRUE)
theme_minimal2 <- theme_minimal() %>%  theme_set()
theme_minimal2 <-
  theme_update(
    panel.border = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
    ,strip.background = element_rect(
      linetype = "solid"
      ,colour = "grey92"
      ,fill = NA
    )
  )
```
\[
  \newcommand{\Prob}{\operatorname{P}}
  \newcommand{\E}{\operatorname{E}}
  \newcommand{\Var}{\operatorname{Var}}
  \newcommand{\Cov}{\operatorname{Cov}}
  \newcommand{\se}{\operatorname{se}}
  \newcommand{\re}{\operatorname{re}}
  \newcommand{\ybar}{{\overline{Y}}}
  \newcommand{\phat}{{\hat{p}}}
  \newcommand{\that}{{\hat{T}}}
  \newcommand{\med}{{\tilde{Y}}}
\]

## 4.13
```{r 4.13}
death_penalty_ct <-
  array(
    data =
      c(
         6, 11, 103-6, 63-11
        ,0, 19, 9-0, 151-19
      )
    ,dim = c(2, 2, 2)
    ,dimnames = list(
      Victim = c("B", "W")
      ,Result = c("D", "A")
      ,Killer = c("B", "W")
    )
  )
death_penalty_ct
mantelhaen.test(death_penalty_ct, correct = FALSE)
```
### a.
$$
  CMH \sim {\chi_{1}}^{2}
$$
```{r 4.13a}
pchisq(q = 7, df = 1, lower.tail = FALSE)
```

### b.
$$
  O_{112} = 0
$$
```{r 4.13b}
sum(death_penalty_ct["B",,"W"])*sum(death_penalty_ct[,"D","W"])/sum(death_penalty_ct[,,"W"])
```

This, coupled with the results of the test, shows that what was observed was significantly different than what we'd expect to observe under the null hypothesis.


## 4.15
```{r 4.15}
agri_pay_ct <- 
  array(
    data = 
      c(
         24, 47, 9, 12
        ,10, 45, 3, 8
        ,5, 57, 4, 9
        ,16, 54, 7, 10
        ,7, 59, 4, 12
      )
    ,dim = c(2, 2, 5)
    ,dimnames = list(
      Race = c("B", "W")
      ,Merit_Pay = c("Y", "N")
      ,District = c("NC", "NE", "NW", "SE", "SW")
    )
  )
agri_pay_ct
```
### a.
```{r 4.15a}
mantelhaen.test(agri_pay_ct, correct = FALSE)
```
$$
  H_{0}: \text{Merit Pay is independent of Race, conditionally by district}
$$
$$
  H_{1}: \text{Merit Pay is dependent on Race, conditionally by district}
$$

Since there is a p-value of `r round(mantelhaen.test(agri_pay_ct, correct = FALSE)$p.value, 4)`, we reject the null hypothesis, and accept the alternative based on the sample.

### b.
```{r 4.15b}
agri_pay_data <- 
  tibble(
    District = 
      as.factor(
        rep(
          c("NC", "NE", "NW", "SE", "SW")
          ,times = 
            c(
              sum(agri_pay_ct[,,"NC"])
              ,sum(agri_pay_ct[,,"NE"])
              ,sum(agri_pay_ct[,,"NW"])
              ,sum(agri_pay_ct[,,"SE"])
              ,sum(agri_pay_ct[,,"SW"])
            )
        )
      )
    ,Race =
      as.factor(
        rep(
          rep(c("B", "W"), times = 5)
          ,times =
            c(
              sum(agri_pay_ct["B",,"NC"])
              ,sum(agri_pay_ct["W",,"NC"])
              ,sum(agri_pay_ct["B",,"NE"])
              ,sum(agri_pay_ct["W",,"NE"])
              ,sum(agri_pay_ct["B",,"NW"])
              ,sum(agri_pay_ct["W",,"NW"])
              ,sum(agri_pay_ct["B",,"SE"])
              ,sum(agri_pay_ct["W",,"SE"])
              ,sum(agri_pay_ct["B",,"SW"])
              ,sum(agri_pay_ct["W",,"SW"])
            )
        )
      )
    ,`Merit Pay` =
      rep(
        rep(c(1, 0), times = 10)
        ,times =
          c(
            sum(agri_pay_ct["B","Y","NC"])
            ,sum(agri_pay_ct["B","N","NC"])
            ,sum(agri_pay_ct["W","Y","NC"])
            ,sum(agri_pay_ct["W","N","NC"])
            ,sum(agri_pay_ct["B","Y","NE"])
            ,sum(agri_pay_ct["B","N","NE"])
            ,sum(agri_pay_ct["W","Y","NE"])
            ,sum(agri_pay_ct["W","N","NE"])
            ,sum(agri_pay_ct["B","Y","NW"])
            ,sum(agri_pay_ct["B","N","NW"])
            ,sum(agri_pay_ct["W","Y","NW"])
            ,sum(agri_pay_ct["W","N","NW"])
            ,sum(agri_pay_ct["B","Y","SE"])
            ,sum(agri_pay_ct["B","N","SE"])
            ,sum(agri_pay_ct["W","Y","SE"])
            ,sum(agri_pay_ct["W","N","SE"])
            ,sum(agri_pay_ct["B","Y","SW"])
            ,sum(agri_pay_ct["B","N","SW"])
            ,sum(agri_pay_ct["W","Y","SW"])
            ,sum(agri_pay_ct["W","N","SW"])
          )
      )
  )
agri_pay_data %>% 
  group_by(District, Race) %>% 
  summarize(Yes = sum(`Merit Pay`))
logit_agri <- 
  agri_pay_data %>% 
  glm(
    `Merit Pay` ~ District + Race
    ,family = binomial
    ,data = .
  )
summary(logit_agri)
```

We can read the p-value from the table for Race (0.00555), which tests the same null and alternative hypothesis as stated in a. This is because that p-value represents:
$$
  H_{0}: \beta_{Race} = 0
$$
$$
  H_{1}: \beta_{Race} \neq 0
$$

Both of which are conditional on District (holding District constant).

### c.
The information we can glean from a model based analysis, is that we can determine the effect size and direction of Race, holding District constant, as opposed to whether or not an effect exists.


## 5.03
### a.
```{r 5.03a}
pchisq(q = 173.68 - 170.44, df = 155 - 152, lower.tail = FALSE)
```

Since this is $>\alpha=0.05$, we can remove the 2nd order interaction term from the model.

### b.
Without seeing the regression output, I'd assume the $S*W$ term has the highest p-value, and therefore would be dropped next. As well, it has the highest AIC of the 3 models.

### c.
I would go to model 4b $(W+C*S)$ as it has the highest AIC of the two models.

### d.
```{r 5.03d}
pchisq(q = 186.61 - 177.61, df = 166 - 160, lower.tail = FALSE)
```

Since this is $>\alpha=0.05$, we can remove the 1st order interaction term from the model.

### e.
$C+S+W$ is preferred, as it has the smallest AIC.


## 5.05
The model with the 4 main predictors is the preferred model, as it has the smallest AIC (637.5), of the 4 models.


## 5.07
### a.
Let: $\theta$ be the odds in favor of smoking.

\begin{equation}
  \ln(\theta) =
  \hat{\beta}_{0}
\end{equation}

\begin{equation}
  \ln(\theta) =
  \hat{\beta}_{0} + \hat{\beta}_{1}E/I + \hat{\beta}_{2}S/N + \hat{\beta}_{3}T/F + \hat{\beta}_{4}J/P
\end{equation}

$$
  \ln(\theta) =
  \hat{\beta}_{0} + \hat{\beta}_{1}E/I + \hat{\beta}_{2}S/N + \hat{\beta}_{3}T/F + \hat{\beta}_{4}J/P +
  \hat{\beta}_{5}E/I\times S/N + \hat{\beta}_{6}E/I\times T/F +
$$
\begin{equation}
  \hat{\beta}_{7}E/I\times J/P + \hat{\beta}_{8}S/N\times T/F + \hat{\beta}_{9}S/N\times J/P + 
  \hat{\beta}_{10}T/F\times J/P
\end{equation}

$$
  \ln(\theta) =
  \hat{\beta}_{0} + \hat{\beta}_{1}E/I + \hat{\beta}_{2}S/N + \hat{\beta}_{3}T/F + \hat{\beta}_{4}J/P +
  \hat{\beta}_{5}E/I\times S/N + \hat{\beta}_{6}E/I\times T/F +
$$
$$
  \hat{\beta}_{7}E/I\times J/P + \hat{\beta}_{8}S/N\times T/F + \hat{\beta}_{9}S/N\times J/P +
  \hat{\beta}_{10}T/F\times J/P + \hat{\beta}_{11}E/I\times S/N\times T/F +
  \hat{\beta}_{12}E/I\times S/N\times J/P +
$$
\begin{equation}
  \hat{\beta}_{13}E/I\times T/F\times J/P + \hat{\beta}_{14}S/N\times T/F\times J/P
\end{equation}

### b.
$$
  AIC_{(1)} = 2(1) + 1130.23 = 1132.23,
  AIC_{(2)} = 2(5) + 1124.86 = 1134.86
$$
$$
  AIC_{(3)} = 2(11) + 1119.87 = 1141.87,
  AIC_{(4)} = 2(15) + 1116.47 = 1146.47
$$

Since (1) has the smallest AIC value, it is the preferred model.

### c.
Probably not, as neither the sensitivity nor the specificity are particularly high, implying that the model doesn't have a high True Positive Rate, or high True Negative Rate. As well, with an AUC of 0.55, our model is only slightly better than randomly guessing. As such, knowledge of a person's personality type does not help you predict whether or not someone is a frequent smoker.


## 5.15
```{r 5.15}
miss_person_ct <- 
  array(
    data =
      c(
        33, 38, 3271-33, 2486-38,
        63, 108, 7256-63, 8877-108,
        157, 159, 5065-157, 3520-159
      )
    ,dim = c(2, 2, 3)
    ,dimnames = list(
      Gender = c("M", "F")
      ,Result = c("Missing", "Found")
      ,Age = c("<=13", "14-18", ">=19")
    )
  )
miss_person_ct

mantelhaen.test(miss_person_ct, correct = FALSE)

miss_person_data <-
  tibble(
    Age =
      as.factor(
        rep(
          x = c("<=13", "14-18", ">=19")
          ,times =
            c(
              sum(miss_person_ct[,,"<=13"])
              ,sum(miss_person_ct[,,"14-18"])
              ,sum(miss_person_ct[,,">=19"])
            )
        )
      )
    ,Gender =
      as.factor(
        rep(
          x = 
            rep(x = c("M", "F"), times = 3)
          ,times = 
            c(
              sum(miss_person_ct["M",,"<=13"])
              ,sum(miss_person_ct["F",,"<=13"])
              ,sum(miss_person_ct["M",,"14-18"])
              ,sum(miss_person_ct["F",,"14-18"])
              ,sum(miss_person_ct["M",,">=19"])
              ,sum(miss_person_ct["F",,">=19"])
            )
        )
      )
    ,Result =
      rep(
        x =
          rep(x = c(1, 0), times = 6)
        ,times =
          c(
            sum(miss_person_ct["M","Missing","<=13"])
            ,sum(miss_person_ct["M","Found","<=13"])
            ,sum(miss_person_ct["F","Missing","<=13"])
            ,sum(miss_person_ct["F","Found","<=13"])
            ,sum(miss_person_ct["M","Missing","14-18"])
            ,sum(miss_person_ct["M","Found","14-18"])
            ,sum(miss_person_ct["F","Missing","14-18"])
            ,sum(miss_person_ct["F","Found","14-18"])
            ,sum(miss_person_ct["M","Missing",">=19"])
            ,sum(miss_person_ct["M","Found",">=19"])
            ,sum(miss_person_ct["F","Missing",">=19"])
            ,sum(miss_person_ct["F","Found",">=19"])
          )
      )
  )

miss_person_summary <- 
  miss_person_data %>%
  group_by(Age, Gender) %>%
  summarize(
    Missing = sum(Result)
    ,Found = length(Result) - sum(Result)
    ,Total = length(Result)
  ) %>% 
  mutate(Proportion = Missing/Total)
miss_person_summary

logit_miss_person <- 
  miss_person_data %>% 
  glm(
    Result ~ Age + Gender
    ,family = binomial
    ,data = .
  )
summary(logit_miss_person)

miss_person_summary %<>% 
  cbind(
    phat = 
      predict(
        object = logit_miss_person
        ,newdata = miss_person_summary %>% dplyr::select(Age, Gender)
        ,type = "response"
      ) %>% t
  ) %>% 
  mutate(
    Expected_Missing = Total*phat
    ,Expected_Found = Total*(1-phat)
  )
miss_person_summary

chisq_stat <-
  miss_person_summary %>% 
  ungroup %>% 
  mutate(
    ChiSq_Missing = (Missing - Expected_Missing)^(2) / Expected_Missing
    ,ChiSq_Found = (Found - Expected_Found)^(2) / Expected_Found
  ) %>% 
  dplyr::select(
    ChiSq_Missing
    ,ChiSq_Found
  ) %>% 
  sum
chisq_stat
pchisq(q = chisq_stat, df = (2-1)*(2-1)*(3-1), lower.tail = FALSE)

gsq_stat <-
  miss_person_summary %>% 
  ungroup %>% 
  mutate(
    GSq_Missing = 2*Missing*log(Missing/Expected_Missing)
    ,GSq_Found = 2*Found*log(Found/Expected_Found)
  ) %>% 
  dplyr::select(
    GSq_Missing
    ,GSq_Found
  ) %>% 
  sum
gsq_stat
pchisq(q = gsq_stat, df = (2-1)*(2-1)*(3-1), lower.tail = FALSE)
```
Let $\theta:$ be the odds in favour of staying missing after a year.
$$
  x_{1} =
  \begin{cases}
    1, Age\geq19  \\
    0, Age<19
  \end{cases}
$$
$$
  x_{2} =
  \begin{cases}
    1, 14<Age<18 \\
    0, Age<14 \cup Age>18
  \end{cases}
$$
$$
  x_{3} =
  \begin{cases}
    1, Male \\
    0, Female
  \end{cases}
$$
$$
  \ln(\theta) = -4.1845 + 1.1279x_{1} - 0.1980x_{2} - 0.3803x_{3}
$$

The log odds in favour increase on average by 1.1279, if the person is 19 or older, decrease on average by 0.1980 if they are between 14 and 18, and decrease on average by 0.3803 if they are male.

The Cochran-Mantel-Haenszel Test yields a p-value of $1.085\times10^{-5}<\alpha=0.05$, rejecting the null hypothesis of independence, contingent on Age. Based on the sample assume some dependency exists between Gender and Missing Status, contingent on Age.

Both the $\chi^{2}$ and $G^{2}$ yield values of $\approx0.1$, giving p-values of $>0.95$, therefore failing to reject the null hypothesis that the model is a good fit for the data, based on the sample.