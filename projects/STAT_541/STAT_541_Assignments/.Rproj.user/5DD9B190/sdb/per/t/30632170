{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Week 07 Questions\"\nauthor: \"Scott Graham\"\ndate: \"October 27, 2017\"\noutput:\n  html_document:\n    theme: lumen\n    toc: true\n    toc_depth: 2\n    toc_float: true\n    code_folding: hide\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(magrittr, warn.conflicts = FALSE, quietly = TRUE)\ntheme_minimal2 <- theme_minimal() %>%  theme_set()\ntheme_minimal2 <-\n  theme_update(\n    panel.border = element_rect(\n      linetype = \"solid\"\n      ,colour = \"grey92\"\n      ,fill = NA\n    )\n    ,strip.background = element_rect(\n      linetype = \"solid\"\n      ,colour = \"grey92\"\n      ,fill = NA\n    )\n  )\n```\n\\[\n  \\newcommand{\\Prob}{\\operatorname{P}}\n  \\newcommand{\\E}{\\operatorname{E}}\n  \\newcommand{\\Var}{\\operatorname{Var}}\n  \\newcommand{\\Cov}{\\operatorname{Cov}}\n  \\newcommand{\\se}{\\operatorname{se}}\n  \\newcommand{\\re}{\\operatorname{re}}\n  \\newcommand{\\ybar}{{\\overline{Y}}}\n  \\newcommand{\\phat}{{\\hat{p}}}\n  \\newcommand{\\that}{{\\hat{T}}}\n  \\newcommand{\\med}{{\\tilde{Y}}}\n\\]\n\n## 4.13\n```{r 4.13}\ndeath_penalty_ct <-\n  array(\n    data =\n      c(\n         6, 11, 103-6, 63-11\n        ,0, 19, 9-0, 151-19\n      )\n    ,dim = c(2, 2, 2)\n    ,dimnames = list(\n      Victim = c(\"B\", \"W\")\n      ,Result = c(\"D\", \"A\")\n      ,Killer = c(\"B\", \"W\")\n    )\n  )\ndeath_penalty_ct\nmantelhaen.test(death_penalty_ct, correct = FALSE)\n```\n### a.\n$$\n  CMH \\sim {\\chi_{1}}^{2}\n$$\n```{r 4.13a}\npchisq(q = 7, df = 1, lower.tail = FALSE)\n```\n\n### b.\n$$\n  O_{112} = 0\n$$\n```{r 4.13b}\nsum(death_penalty_ct[\"B\",,\"W\"])*sum(death_penalty_ct[,\"D\",\"W\"])/sum(death_penalty_ct[,,\"W\"])\n```\n\nThis, coupled with the results of the test, shows that what was observed was significantly different than what we'd expect to observe under the null hypothesis.\n\n\n## 4.15\n```{r 4.15}\nagri_pay_ct <- \n  array(\n    data = \n      c(\n         24, 47, 9, 12\n        ,10, 45, 3, 8\n        ,5, 57, 4, 9\n        ,16, 54, 7, 10\n        ,7, 59, 4, 12\n      )\n    ,dim = c(2, 2, 5)\n    ,dimnames = list(\n      Race = c(\"B\", \"W\")\n      ,Merit_Pay = c(\"Y\", \"N\")\n      ,District = c(\"NC\", \"NE\", \"NW\", \"SE\", \"SW\")\n    )\n  )\nagri_pay_ct\n```\n### a.\n```{r 4.15a}\nmantelhaen.test(agri_pay_ct, correct = FALSE)\n```\n$$\n  H_{0}: \\text{Merit Pay is independent of Race, conditionally by district}\n$$\n$$\n  H_{1}: \\text{Merit Pay is dependent on Race, conditionally by district}\n$$\n\nSince there is a p-value of `r round(mantelhaen.test(agri_pay_ct, correct = FALSE)$p.value, 4)`, we reject the null hypothesis, and accept the alternative based on the sample.\n\n### b.\n```{r 4.15b}\nagri_pay_data <- \n  tibble(\n    District = \n      as.factor(\n        rep(\n          c(\"NC\", \"NE\", \"NW\", \"SE\", \"SW\")\n          ,times = \n            c(\n              sum(agri_pay_ct[,,\"NC\"])\n              ,sum(agri_pay_ct[,,\"NE\"])\n              ,sum(agri_pay_ct[,,\"NW\"])\n              ,sum(agri_pay_ct[,,\"SE\"])\n              ,sum(agri_pay_ct[,,\"SW\"])\n            )\n        )\n      )\n    ,Race =\n      as.factor(\n        rep(\n          rep(c(\"B\", \"W\"), times = 5)\n          ,times =\n            c(\n              sum(agri_pay_ct[\"B\",,\"NC\"])\n              ,sum(agri_pay_ct[\"W\",,\"NC\"])\n              ,sum(agri_pay_ct[\"B\",,\"NE\"])\n              ,sum(agri_pay_ct[\"W\",,\"NE\"])\n              ,sum(agri_pay_ct[\"B\",,\"NW\"])\n              ,sum(agri_pay_ct[\"W\",,\"NW\"])\n              ,sum(agri_pay_ct[\"B\",,\"SE\"])\n              ,sum(agri_pay_ct[\"W\",,\"SE\"])\n              ,sum(agri_pay_ct[\"B\",,\"SW\"])\n              ,sum(agri_pay_ct[\"W\",,\"SW\"])\n            )\n        )\n      )\n    ,`Merit Pay` =\n      rep(\n        rep(c(1, 0), times = 10)\n        ,times =\n          c(\n            sum(agri_pay_ct[\"B\",\"Y\",\"NC\"])\n            ,sum(agri_pay_ct[\"B\",\"N\",\"NC\"])\n            ,sum(agri_pay_ct[\"W\",\"Y\",\"NC\"])\n            ,sum(agri_pay_ct[\"W\",\"N\",\"NC\"])\n            ,sum(agri_pay_ct[\"B\",\"Y\",\"NE\"])\n            ,sum(agri_pay_ct[\"B\",\"N\",\"NE\"])\n            ,sum(agri_pay_ct[\"W\",\"Y\",\"NE\"])\n            ,sum(agri_pay_ct[\"W\",\"N\",\"NE\"])\n            ,sum(agri_pay_ct[\"B\",\"Y\",\"NW\"])\n            ,sum(agri_pay_ct[\"B\",\"N\",\"NW\"])\n            ,sum(agri_pay_ct[\"W\",\"Y\",\"NW\"])\n            ,sum(agri_pay_ct[\"W\",\"N\",\"NW\"])\n            ,sum(agri_pay_ct[\"B\",\"Y\",\"SE\"])\n            ,sum(agri_pay_ct[\"B\",\"N\",\"SE\"])\n            ,sum(agri_pay_ct[\"W\",\"Y\",\"SE\"])\n            ,sum(agri_pay_ct[\"W\",\"N\",\"SE\"])\n            ,sum(agri_pay_ct[\"B\",\"Y\",\"SW\"])\n            ,sum(agri_pay_ct[\"B\",\"N\",\"SW\"])\n            ,sum(agri_pay_ct[\"W\",\"Y\",\"SW\"])\n            ,sum(agri_pay_ct[\"W\",\"N\",\"SW\"])\n          )\n      )\n  )\nagri_pay_data %>% \n  group_by(District, Race) %>% \n  summarize(Yes = sum(`Merit Pay`))\nlogit_agri <- \n  agri_pay_data %>% \n  glm(\n    `Merit Pay` ~ District + Race\n    ,family = binomial\n    ,data = .\n  )\nsummary(logit_agri)\n```\n\nWe can read the p-value from the table for Race (0.00555), which tests the same null and alternative hypothesis as stated in a. This is because that p-value represents:\n$$\n  H_{0}: \\beta_{Race} = 0\n$$\n$$\n  H_{1}: \\beta_{Race} \\neq 0\n$$\n\nBoth of which are conditional on District (holding District constant).\n\n### c.\nThe information we can glean from a model based analysis, is that we can determine the effect size and direction of Race, holding District constant, as opposed to whether or not an effect exists.\n\n\n## 5.03\n### a.\n```{r 5.03a}\npchisq(q = 173.68 - 170.44, df = 155 - 152, lower.tail = FALSE)\n```\n\nSince this is $>\\alpha=0.05$, we can remove the 2nd order interaction term from the model.\n\n### b.\nWithout seeing the regression output, I'd assume the $S*W$ term has the highest p-value, and therefore would be dropped next. As well, it has the highest AIC of the 3 models.\n\n### c.\nI would go to model 4b $(W+C*S)$ as it has the highest AIC of the two models.\n\n### d.\n```{r 5.03d}\npchisq(q = 186.61 - 177.61, df = 166 - 160, lower.tail = FALSE)\n```\n\nSince this is $>\\alpha=0.05$, we can remove the 1st order interaction term from the model.\n\n### e.\n$C+S+W$ is preferred, as it has the smallest AIC.\n\n\n## 5.05\nThe model with the 4 main predictors is the preferred model, as it has the smallest AIC (637.5), of the 4 models.\n\n\n## 5.07\nLet: $\\theta$ be the odds in favor of smoking.\n\n\\begin{equation}\n  \\ln(\\theta) =\n  \\hat{\\beta}_{0}\n\\end{equation}\n\n\\begin{equation}\n  \\ln(\\theta) =\n  \\hat{\\beta}_{0} + \\hat{\\beta}_{1}E/I + \\hat{\\beta}_{2}S/N + \\hat{\\beta}_{3}T/F + \\hat{\\beta}_{4}J/P\n\\end{equation}\n\n$$\n  \\ln(\\theta) =\n  \\hat{\\beta}_{0} + \\hat{\\beta}_{1}E/I + \\hat{\\beta}_{2}S/N + \\hat{\\beta}_{3}T/F + \\hat{\\beta}_{4}J/P +\n  \\hat{\\beta}_{5}E/I\\times S/N + \\hat{\\beta}_{6}E/I\\times T/F +\n$$\n\\begin{equation}\n  \\hat{\\beta}_{7}E/I\\times J/P + \\hat{\\beta}_{8}S/N\\times T/F + \\hat{\\beta}_{9}S/N\\times J/P + \n  \\hat{\\beta}_{10}T/F\\times J/P\n\\end{equation}\n\n$$\n  \\ln(\\theta) =\n  \\hat{\\beta}_{0} + \\hat{\\beta}_{1}E/I + \\hat{\\beta}_{2}S/N + \\hat{\\beta}_{3}T/F + \\hat{\\beta}_{4}J/P +\n  \\hat{\\beta}_{5}E/I\\times S/N + \\hat{\\beta}_{6}E/I\\times T/F +\n$$\n$$\n  \\hat{\\beta}_{7}E/I\\times J/P + \\hat{\\beta}_{8}S/N\\times T/F + \\hat{\\beta}_{9}S/N\\times J/P +\n  \\hat{\\beta}_{10}T/F\\times J/P + \\hat{\\beta}_{11}E/I\\times S/N\\times T/F +\n  \\hat{\\beta}_{12}E/I\\times S/N\\times J/P +\n$$\n\\begin{equation}\n  \\hat{\\beta}_{13}E/I\\times T/F\\times J/P + \\hat{\\beta}_{14}S/N\\times T/F\\times J/P\n\\end{equation}\n\n### b.\n$$\n  AIC_{(1)} = 2(1) + 1130.23 = 1132.23,\n  AIC_{(2)} = 2(5) + 1124.86 = 1134.86\n$$\n$$\n  AIC_{(3)} = 2(11) + 1119.87 = 1141.87,\n  AIC_{(4)} = 2(15) + 1116.47 = 1146.47\n$$\n\nSince (1) has the smallest AIC value, it is the preferred model.\n\n### c.\nProbably not, as neither the sensitivity nor the specificity are particularly high, implying that the model doesn't have a high True Positive Rate, or high True Negative Rate. As well, with an AUC of 0.55, our model is only slightly better than randomly guessing. As such, knowledge of a person's personality type does not help you predict whether or not someone is a frequent smoker.\n\n\n## 5.15\n```{r 5.15}\nmiss_person_ct <- \n  array(\n    data =\n      c(\n        33, 38, 3271-33, 2486-38,\n        63, 108, 7256-63, 8877-108,\n        157, 159, 5065-157, 3520-159\n      )\n    ,dim = c(2, 2, 3)\n    ,dimnames = list(\n      Gender = c(\"M\", \"F\")\n      ,Result = c(\"Missing\", \"Found\")\n      ,Age = c(\"<=13\", \"14-18\", \">=19\")\n    )\n  )\nmiss_person_ct\n\nmantelhaen.test(miss_person_ct, correct = FALSE)\n\nmiss_person_data <-\n  tibble(\n    Age =\n      as.factor(\n        rep(\n          x = c(\"<=13\", \"14-18\", \">=19\")\n          ,times =\n            c(\n              sum(miss_person_ct[,,\"<=13\"])\n              ,sum(miss_person_ct[,,\"14-18\"])\n              ,sum(miss_person_ct[,,\">=19\"])\n            )\n        )\n      )\n    ,Gender =\n      as.factor(\n        rep(\n          x = \n            rep(x = c(\"M\", \"F\"), times = 3)\n          ,times = \n            c(\n              sum(miss_person_ct[\"M\",,\"<=13\"])\n              ,sum(miss_person_ct[\"F\",,\"<=13\"])\n              ,sum(miss_person_ct[\"M\",,\"14-18\"])\n              ,sum(miss_person_ct[\"F\",,\"14-18\"])\n              ,sum(miss_person_ct[\"M\",,\">=19\"])\n              ,sum(miss_person_ct[\"F\",,\">=19\"])\n            )\n        )\n      )\n    ,Result =\n      rep(\n        x =\n          rep(x = c(1, 0), times = 6)\n        ,times =\n          c(\n            sum(miss_person_ct[\"M\",\"Missing\",\"<=13\"])\n            ,sum(miss_person_ct[\"M\",\"Found\",\"<=13\"])\n            ,sum(miss_person_ct[\"F\",\"Missing\",\"<=13\"])\n            ,sum(miss_person_ct[\"F\",\"Found\",\"<=13\"])\n            ,sum(miss_person_ct[\"M\",\"Missing\",\"14-18\"])\n            ,sum(miss_person_ct[\"M\",\"Found\",\"14-18\"])\n            ,sum(miss_person_ct[\"F\",\"Missing\",\"14-18\"])\n            ,sum(miss_person_ct[\"F\",\"Found\",\"14-18\"])\n            ,sum(miss_person_ct[\"M\",\"Missing\",\">=19\"])\n            ,sum(miss_person_ct[\"M\",\"Found\",\">=19\"])\n            ,sum(miss_person_ct[\"F\",\"Missing\",\">=19\"])\n            ,sum(miss_person_ct[\"F\",\"Found\",\">=19\"])\n          )\n      )\n  )\n\nmiss_person_summary <- \n  miss_person_data %>%\n  group_by(Age, Gender) %>%\n  summarize(\n    Missing = sum(Result)\n    ,Found = length(Result) - sum(Result)\n    ,Total = length(Result)\n  ) %>% \n  mutate(Proportion = Missing/Total)\nmiss_person_summary\n\nlogit_miss_person <- \n  miss_person_data %>% \n  glm(\n    Result ~ Age + Gender\n    ,family = binomial\n    ,data = .\n  )\nsummary(logit_miss_person)\n\nmiss_person_summary %<>% \n  cbind(\n    phat = \n      predict(\n        object = logit_miss_person\n        ,newdata = miss_person_summary %>% dplyr::select(Age, Gender)\n        ,type = \"response\"\n      ) %>% t\n  ) %>% \n  mutate(\n    Expected_Missing = Total*phat\n    ,Expected_Found = Total*(1-phat)\n  )\nmiss_person_summary\n\nchisq_stat <-\n  miss_person_summary %>% \n  ungroup %>% \n  mutate(\n    ChiSq_Missing = (Missing - Expected_Missing)^(2) / Expected_Missing\n    ,ChiSq_Found = (Found - Expected_Found)^(2) / Expected_Found\n  ) %>% \n  dplyr::select(\n    ChiSq_Missing\n    ,ChiSq_Found\n  ) %>% \n  sum\nchisq_stat\npchisq(q = chisq_stat, df = (2-1)*(2-1)*(3-1), lower.tail = FALSE)\n\ngsq_stat <-\n  miss_person_summary %>% \n  ungroup %>% \n  mutate(\n    GSq_Missing = 2*Missing*log(Missing/Expected_Missing)\n    ,GSq_Found = 2*Found*log(Found/Expected_Found)\n  ) %>% \n  dplyr::select(\n    GSq_Missing\n    ,GSq_Found\n  ) %>% \n  sum\ngsq_stat\npchisq(q = gsq_stat, df = (2-1)*(2-1)*(3-1), lower.tail = FALSE)\n```\nLet $\\theta:$ be the odds in favour of staying missing after a year.\n$$\n  x_{1} =\n  \\begin{cases}\n    1, Age\\geq19  \\\\\n    0, Age<19\n  \\end{cases}\n$$\n$$\n  x_{2} =\n  \\begin{cases}\n    1, 14<Age<18 \\\\\n    0, Age<14 \\cup Age>18\n  \\end{cases}\n$$\n$$\n  x_{3} =\n  \\begin{cases}\n    1, Male \\\\\n    0, Female\n  \\end{cases}\n$$\n$$\n  \\ln(\\theta) = -4.1845 + 1.1279x_{1} - 0.1980x_{2} - 0.3803x_{3}\n$$\n\nThe log odds in favour increase on average by 1.1279, if the person is 19 or older, decrease on average by 0.1980 is they are 13 or younger, and decrease on average by 0.3803 if they are male.\n\nThe Cochran-Mantel-Haenszel Test yields a p-value of $1.085\\times10^{-5}<\\alpha=0.05$, rejecting the null hypothesis of independence, contingent on Age. Based on the sample assume some dependency exists between Gender and Missing Status, contingent on Age.\n\nBoth the $\\chi^{2}$ and $G^{2}$ yield values of $\\approx0.1$, giving p-values of $>0.95$, therefore failing to reject the null hypothesis that the model is a good fit for the data, based on the sample.",
    "created" : 1509062390031.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3808402364",
    "id" : "30632170",
    "lastKnownWriteTime" : 1509064024,
    "last_content_update" : 1509064024,
    "path" : "~/GitHub/grahamStat/projects/STAT_541/STAT_541_Assignments/Week_07_Questions.Rmd",
    "project_path" : "Week_07_Questions.Rmd",
    "properties" : {
    },
    "relative_order" : 8,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}