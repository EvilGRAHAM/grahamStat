{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Week 04 Questions\"\nauthor: \"Scott Graham\"\ndate: \"October 06, 2017\"\noutput:\n  html_document:\n    theme: lumen\n    toc: true\n    toc_depth: 2\n    toc_float: true\n    code_folding: hide\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nlibrary(tidyverse, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(magrittr, warn.conflicts = FALSE, quietly = TRUE)\nlibrary(MASS, warn.conflicts = FALSE, quietly = TRUE)\ntheme_minimal2 <- theme_minimal() %>%  theme_set()\ntheme_minimal2 <-\n  theme_update(\n    panel.border = element_rect(\n      linetype = \"solid\"\n      ,colour = \"grey92\"\n      ,fill = NA\n    )\n    ,strip.background = element_rect(\n      linetype = \"solid\"\n      ,colour = \"grey92\"\n      ,fill = NA\n    )\n  )\n```\n\\[\n  \\newcommand{\\Prob}{\\operatorname{P}}\n  \\newcommand{\\E}{\\operatorname{E}}\n  \\newcommand{\\Var}{\\operatorname{Var}}\n  \\newcommand{\\Cov}{\\operatorname{Cov}}\n  \\newcommand{\\se}{\\operatorname{se}}\n  \\newcommand{\\re}{\\operatorname{re}}\n  \\newcommand{\\ybar}{{\\overline{Y}}}\n  \\newcommand{\\phat}{{\\hat{p}}}\n  \\newcommand{\\that}{{\\hat{T}}}\n  \\newcommand{\\med}{{\\tilde{Y}}}\n\\]\n\n## 3.11\n```{r 3.11, include=FALSE}\nwaifer_imperfections <- tibble(\n  Treatment = c(rep (c(\"A\", \"B\"), each = 10))\n  ,Imperfections = c(\n    8\n    ,7\n    ,6\n    ,6\n    ,3\n    ,4\n    ,7\n    ,2\n    ,3\n    ,4\n    ,9\n    ,9\n    ,8\n    ,14\n    ,8\n    ,13\n    ,11\n    ,5\n    ,7\n    ,6\n  )\n)\n```\n### a.\n$$\n  \\ln\\left( \\mu \\right) =\n  \\alpha + \\beta x \\implies\n  \\begin{cases}\n    \\stackrel{x=1}{\\implies}\n    \\ln\\left( \\mu_{B} \\right) =\n    \\alpha + \\beta \\\\\n    \\stackrel{x=0}{\\implies}\n    \\ln\\left( \\mu_{A} \\right) =\n    \\alpha\n  \\end{cases} \\implies\n  \\ln\\left( \\mu_{B} \\right) - \\beta =\n  \\ln\\left( \\mu_{A} \\right) \\implies\n$$\n$$\n  \\beta =\n  \\ln\\left( \\mu_{B} \\right) - \\ln\\left( \\mu_{A} \\right) =\n  \\ln\\left( \\frac{\\mu_{B}}{\\mu_{A}} \\right) \\implies\n  e^{\\beta} =\n  \\frac{\\mu_{B}}{\\mu_{A}}\n$$\n\n### b.\n```{r 3.11b}\npoiss_reg_imperfection <- \n  waifer_imperfections %>% \n  glm(Imperfections ~ Treatment, data = ., family = poisson)\nsummary(poiss_reg_imperfection)\n```\n$$\n  \\ln\\left( \\mu \\right) =\n  1.6094 + 0.5878x \\implies\n  \\mu = 4.99981(1.800024)^{x}\n$$\nThat is, on average for as you move from treatment A to B, $\\ln\\left( \\mu \\right)$ increases by 0.5878, or the mean number of imperfections increases by a multiplicative factor of 1.800024, based on the sample.\n\n### c.\nFrom above, we have a Wald P-Value of $0.000861 < \\alpha = 0.05$, and therefore reject the null hypothesis of equal means, based on the sample, and accept that there is a difference in means between the two treatments.\n\n### d.\n```{r 3.11d}\npoiss_reg_imperfection %>% \n  confint(parm = \"TreatmentB\", level = 0.95) %>% \n  exp\n```\nThat is, based on the sample, with 95\\% confidence, the true value of $\\frac{\\mu_{B}}{\\mu_{A}}$ lies somewhere between 1.28 and 2.56.\n\n\n## 3.15\n### a.\n$$\n  \\hat{\\mu}_{1} = \n  e^{-2.38+1.733(1)} =\n  e^{-0.657} =\n  0.5236\n$$\n$$\n  \\hat{\\mu}_{2} = \n  e^{-2.38+1.733(0)} =\n  e^{-2.38} =\n  0.0925\n$$\n\n### b.\n95\\% CI for $\\mu_{1}/\\mu_{2}$:\n$$\n  e^{\\hat{\\beta}\\mp z_{0.975}\\se\\left(\\hat{\\beta}\\right)}\n$$\n```{r 3.15b}\nexp(1.733+qnorm(0.975)*0.147*c(-1, 1))\n```\n\n### c.\nThe negative binomial model appears to be more believable. This is because we'd expect $\\mu_i={\\sigma_{i}}^{2}$, which may be true for whites, but definitely doe not hold for blacks.\n\n### d.\nIf the Poisson model were appropriate, we'd expect to see $\\hat{D}\\approx 0$, but instead we observe a value of 4.94. Since 0 is close to 5 standard deviations away from the estimate, it'd be extremely unlikely this value is merely an outlier.\n\n\n## 3.19\n```{r 3.19, include=FALSE}\ntrain_collisions <- tibble(\n  Year = 1975:2003\n  ,Km = c(\n    436\n    ,426\n    ,425\n    ,430\n    ,426\n    ,430\n    ,417\n    ,372\n    ,401\n    ,389\n    ,418\n    ,414\n    ,397\n    ,443\n    ,436\n    ,431\n    ,439\n    ,430\n    ,425\n    ,415\n    ,423\n    ,437\n    ,463\n    ,487\n    ,505\n    ,503\n    ,508\n    ,516\n    ,518\n  )\n  ,Collisions = c(\n    5\n    ,2\n    ,1\n    ,2\n    ,3\n    ,2\n    ,2\n    ,2\n    ,2\n    ,5\n    ,0\n    ,2\n    ,1\n    ,2\n    ,4\n    ,1\n    ,2\n    ,1\n    ,0\n    ,2\n    ,1\n    ,2\n    ,1\n    ,0\n    ,1\n    ,1\n    ,0\n    ,1\n    ,0\n  )\n  ,Road_Collisions = c(\n    2\n    ,12\n    ,8\n    ,4\n    ,3\n    ,2\n    ,2\n    ,3\n    ,7\n    ,3\n    ,5\n    ,13\n    ,6\n    ,4\n    ,4\n    ,2\n    ,6\n    ,4\n    ,4\n    ,4\n    ,2\n    ,2\n    ,1\n    ,4\n    ,2\n    ,3\n    ,4\n    ,3\n    ,3\n  )\n)\n```\n### a.\n```{r 3.19a}\ntrain_collisions %<>%\n  mutate(\n    Collisions_Total = Collisions + Road_Collisions\n    ,Rate = (Collisions + Road_Collisions)/Km\n    ,Year_base = Year-min(Year)\n  )\npoiss_reg_train <- suppressWarnings(\n  train_collisions %>% \n  glm(Rate ~ Year_base, data = ., family = poisson)\n)\npoiss_reg_train_int <- suppressWarnings(\n  train_collisions %>% \n  glm(Rate ~ 1, data = ., family = poisson)\n)\nsummary(poiss_reg_train)\nsummary(poiss_reg_train_int)\nanova(poiss_reg_train_int, poiss_reg_train, test = \"Chisq\")\n```\n\nFrom our $\\chi^{2}$ test, our p-value is $>\\alpha=0.05$, so we fail to reject the null hypothesis, that the models explain the same amount of variance, based on the sample. As such, we conclude that the Year exhibits no effect, and our rates are constant.\n\n### b.\n$$\n  W =\n  \\frac{-0.0337-0}{0.0130} =\n  -2.592308 \\sim\n  \\mathcal{N}(0,1) \\implies\n  P-Value = 2\\min\\{\\Prob(W \\leq -2.592308), \\Prob(W \\geq -2.592308)\\} =\n$$\n$$\n  2\\min\\{0.004766719, 0.9952333\\} =\n  0.009533438\n$$\nSince this is $<\\alpha=0.05$, we reject our null hypothesis of $\\beta=0$, and assume the alternative to be true based on the sample.\n\n### c.\n95\\% CI for $\\beta$:\n$$\n  [-0.060,-0.008]\n$$\n95\\% CI for $e^{\\beta}$:\n$$\n  e^{[-0.060,-0.008]} =\n  [0.9417645,0.9920319]\n$$\nThat is, with 95\\% confidence, the true average multiplicative increase for the collision rate as the year increases by 1, is between 0.9417645 and 0.9920319, based on the sample.\n\n\n## 4.01\n### a.\n$$\n  \\ln\\left(\\theta\\right) =\n  -3.7771 + 0.1449x \\stackrel{x=8}{\\implies}\n  \\ln\\left(\\theta\\right) =\n  -2.6179 \\implies\n  \\hat{\\pi} =\n  \\frac{e^{-2.6179}}{1+e^{-2.6179}} =\n  0.068\n$$\n\n### b.\n$$\n  \\ln\\left(\\theta\\right) =\n  -3.7771 + 0.1449x \\stackrel{x=26}{\\implies}\n  \\ln\\left(\\theta\\right) =\n  -0.0097 \\implies\n  \\hat{\\pi} =\n  \\frac{e^{-0.0097}}{1+e^{-0.0097}} =\n  0.4975 \\approx\n  0.5\n$$\n\n### c.\n```{r 4.01c}\nprob <- function(x){exp(-3.7771 + 0.1449*x)/(1 + exp(-3.7771 + 0.1449*x))}\n0.1449*prob(8)*(1-prob(8))\n0.1449*prob(26)*(1-prob(26))\nggplot(data = tibble(x = c(0, 75)), aes(x)) + \n  stat_function(fun = prob) + \n  labs(y = expression(hat(pi)))\n```\n\n### d.\n```{r 4.1d}\nprob(28)\nprob(14)\nprob(28)-prob(14)\n```\n\n\n### e.\n$$\n  \\frac{\\theta(x)}{\\theta(x-1)} =\n  \\frac{e^{-3.7771 + 0.1449(x)}}{e^{-3.7771 + 0.1449(x-1)}} =\n  e^{0.1449} =\n  1.16\n$$\n\n\n## 4.03\n### a.\nFor each new decade, the probability of a CG for a pitcher falls by 6.94\\% on average in the NL, based on the sample.\n\n### b.\n$$\n  \\hat{\\pi}(12) = \n  0.7578 - 0.0694(12) = \n  -0.075\n$$\nThis is not plausible, as $\\pi\\in[0,1]$, and -0.075 exists outside of it. As well, this year does not exists in the sample, so it is outside the scope of the regression.\n\n### c.\n$$\n  \\hat{\\pi}(12) = \n  \\frac{e^{1.148 - 0.315(12)}}{1 + e^{1.148 - 0.315(12)}} =\n  0.0671\n$$\nThis is more plausible, as it satisfies our restraint on $\\pi$. However, since 12 is not in the sample, this may not accurate.\n\n\n## 4.05\n```{r 4.05, include=FALSE}\nspace_results <- tibble(\n  Flight = 1:23\n  ,Temp = c(\n    66\n    ,70\n    ,69\n    ,68\n    ,67\n    ,72\n    ,73\n    ,70\n    ,57\n    ,63\n    ,70\n    ,78\n    ,67\n    ,53\n    ,67\n    ,75\n    ,70\n    ,81\n    ,76\n    ,79\n    ,75\n    ,76\n    ,58\n  )\n  ,TD = c(\n    0\n    ,1\n    ,rep(0, times = 6)\n    ,rep(1, times = 3)\n    ,rep(0, times = 2)\n    ,1\n    ,rep(0, times = 6)\n    ,1\n    ,0\n    ,1\n  )\n)\n```\n### a.\n```{r 4.05a}\nlogit_reg_space <- glm(TD ~ Temp, data = space_results, family = binomial)\nsummary(logit_reg_space)\nprob_fun <- function(x){\n  predict.glm(logit_reg_space, newdata = data.frame(Temp = x), type = \"response\")\n}\nspace_results %>% \n  ggplot(\n    aes(\n      x = Temp\n      ,y = TD\n    )\n  ) +\n  geom_point() +\n  stat_smooth(\n    method = \"glm\"\n    ,method.args = list(family = \"binomial\")\n    ,se = FALSE\n  ) +\n  labs(y = expression(hat(pi)))\n```\n\nAs temperature increase, on average the probability of TD decreases, based on the sample.\n\n### b.\n```{r 4.05b}\nprob_fun(31)\n```\n\n### c.\n$$\n  0.5 =\n  \\frac{e^{15.0429-0.2322x}}{1+e^{15.0429-0.2322x}} \\implies\n  \\frac{1}{2} \\left( 1+e^{15.0429-0.2322x} \\right) =\n  e^{15.0429-0.2322x} \\implies\n$$\n$$\n  1 =\n  e^{15.0429-0.2322x} \\implies\n  0 =\n  15.0429-0.2322x \\implies\n  x = 64.79464\n$$\nMore generally:\n$$\n  x_{0.5} =\n  -\\frac{\\alpha}{\\beta}\n$$\n```{r 4.5c}\nTemp_median <- -logit_reg_space$coefficients[[1]]/logit_reg_space$coefficients[[2]]\nTemp_median\nprob_fun(Temp_median)\n```\n\n### d.\nThat is, on average, for every unit increase in temperature in Fahrenheit the log odds in favour decrease by -0.2322, or the odds in favour decrease by a multiplicative factor of 0.7927875, based on the sample.\n\n### e.\n#### i.\nFrom above, our p-value for $H_{0}:\\beta=0 \\text{ vs. } H_{1}:\\beta\\neq0$ is 0.0320, so at $\\alpha=0.05$, we reject the null hypothesis, and assume the alternative to be true based on the sample.\n\n#### ii.\n```{r 4.05dii}\nlogit_reg_space_int <- glm(TD ~ 1, data = space_results, family = binomial)\nsummary(logit_reg_space_int)\nanova(logit_reg_space_int, logit_reg_space, test = \"Chisq\")\n```\n\nFrom above, our p-value for $H_{0}:\\beta=0 \\text{ vs. } H_{1}:\\beta\\neq0$ is 0.004804, so at $\\alpha=0.05$, we reject the null hypothesis, and assume the alternative to be true based on the sample.\n\n\n## 4.07\n```{r 4.07, include=FALSE}\nkyphosis <- tibble(\n  Age = c(\n    12\n    ,15\n    ,42\n    ,52\n    ,59\n    ,73\n    ,82\n    ,91\n    ,96\n    ,105\n    ,114\n    ,120\n    ,121\n    ,128\n    ,130\n    ,139\n    ,139\n    ,157\n    ,1\n    ,1\n    ,2\n    ,8\n    ,11\n    ,18\n    ,22\n    ,31\n    ,37\n    ,61\n    ,72\n    ,81\n    ,97\n    ,112\n    ,118\n    ,127\n    ,131\n    ,140\n    ,151\n    ,159\n    ,177\n    ,206\n  )\n  ,Result = c(\n    rep(1, times = 18)\n    ,rep(0, times = 22)\n  )\n)\n```\n### a.\n```{r 4.07a}\nlogit_reg_kyph <- glm(Result ~ Age, data = kyphosis, family = binomial)\nsummary(logit_reg_kyph)\n```\n\nSince our p-value for age is $0.463>\\alpha=0.05$, we fail to reject the null hypothesis, based on the sample, and assume age has no effect on the result.\n\n### b.\n```{r 4.07b}\nkyphosis %>%\n  ggplot(\n    aes(\n      x = Age\n      ,y = Result\n    )\n  ) +\n  geom_point() +\n  stat_smooth(\n    method = \"glm\"\n    ,method.args = list(family = \"binomial\")\n    ,se = FALSE\n  ) +\n  labs(y = expression(hat(pi)))\n```\n\nAs you can see, there seems to be no noticable difference in clustering at the tails for each response based on age. As such it'd ake sense we'd see such a low significance for the slope parameter. There is however some clustering near the middle for the Kyphosis results, so there may be an additional explanatory variable needed, or some polynomial term.\n\n### c.\n```{r 4.07c}\nkyphosis %<>%\n  mutate(Age2 = Age^2)\nlogit_reg_kyph_poly <- glm(Result ~ Age + Age2, data = kyphosis, family = binomial)\nsummary(logit_reg_kyph_poly)\n\nprob_kyph_poly_fun <- function(x){\n  predict(\n    object = logit_reg_kyph_poly\n    ,newdata = tibble(Age = x, Age2 = x^2)\n    ,type = \"response\"\n  )\n} \n\nkyphosis %>% \n  ggplot() +\n  geom_point(\n    aes(\n      x = Age\n      ,y = Result\n    )\n  ) +\n  stat_function(\n    data = tibble(x = c(0, 200))\n    ,aes(x)\n    ,fun = prob_kyph_poly_fun\n  ) +\n  labs(y = expression(hat(pi)))\n```\n\nThis function is maximized at $\\approx 91.552$, so at Age in months increases up until that point, the probability on average increases, based on the sample. Afterwards as Age increases above that point, on average the probability decreases, based on the sample.",
    "created" : 1509064705486.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "931581982",
    "id" : "CCF6E7EF",
    "lastKnownWriteTime" : 1508527843,
    "last_content_update" : 1508527843,
    "path" : "~/GitHub/grahamStat/projects/STAT_541/STAT_541_Assignments/Week_04_Questions.Rmd",
    "project_path" : "Week_04_Questions.Rmd",
    "properties" : {
    },
    "relative_order" : 7,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}